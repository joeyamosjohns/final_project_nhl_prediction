{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b93318-b204-4321-a902-e019ed05e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###let's try to predeict to two seasons ... \n",
    "\n",
    "##import files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b09481dd-9747-4663-8591-f470375f62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee37b61a-b150-4bf7-b16b-ca7685251210",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd1f830-faa1-449b-99ec-a9dac336049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d23f97-3b91-4868-b566-bed9d7dd76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd905eb-ae20-4c09-a097-d9ced6c6931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a54a87-fb7f-4642-a65e-b2a6c2a6b530",
   "metadata": {},
   "source": [
    "##TUNING INFO \n",
    "\n",
    "\n",
    "##hyper_parameters from here \n",
    "##https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "##for xgboost from here \n",
    "##https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/\n",
    "\n",
    "#xgb\n",
    "\n",
    "trees = [10, 50, 100, 500, 1000, 5000]  #100  #num of trees\n",
    "max_depth = range(1,11)  ##3-5\n",
    "rates = [0.0001, 0.001, 0.01, 0.1, 1.0]  #0.1\n",
    "subsample in arange(0.1, 1.1, 0.1):  #0.4, 0.5  ##this is 0.1, 0.2 ... 1.0 # % of features to sample\n",
    "\n",
    "\n",
    "#svc \n",
    "kernels in [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’] #if you use poly, then adjust degree\n",
    "C in [100, 10, 1.0, 0.1, 0.001]\n",
    "\n",
    "#gb\n",
    "\n",
    "learning_rate in [0.001, 0.01, 0.1]\n",
    "n_estimators [10, 100, 1000]\n",
    "subsample in [0.5, 0.7, 1.0]\n",
    "max_depth in [3, 7, 9]\n",
    "\n",
    "\n",
    "#rfc\n",
    "max_features [1 to 20]  #key\n",
    "max_features in [‘sqrt’, ‘log2’]\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "#bc\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "svm_dic = {'kernels':[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’]}\n",
    "lrc_dic = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "lgr_hp_dic = {'solver': [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’], 'penalty' : [‘none’, ‘l1’, ‘l2’, ‘elasticnet’],\n",
    "'C' :[100, 10, 1.0, 0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bab795dd-c9bf-4524-abdc-c3b9ba2e5f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10385, 26)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Data/Shaped_Data/data_LJ.csv')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "62c4d298-4d0e-4d3d-90f0-61dbc8efd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['goal_diff_target'] = X['home_goals'] - X['away_goals']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ce73f628-e90d-4d24-921f-dd5af5ea3962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'game_id', 'mp_date', 'season', 'home_team', 'away_team',\n",
       "       'home_odds', 'away_odds', 'home_goals', 'away_goals', 'home_win',\n",
       "       'settled_in', 'CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
       "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv',\n",
       "       'goal_diff_target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will use this for correlations etc at the bottom\n",
    "\n",
    "data = X.copy()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7d991107-60b5-4e81-a9ba-12814dfe9269",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "X['season'] = X['season'].apply(int)\n",
    "X['game_id'] = X['game_id'].apply(int)\n",
    "X['mp_date'] = X['mp_date'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "803c4a79-77ee-4168-abfe-ac2fe7ba1b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    1218\n",
       "20142015    1180\n",
       "20152016    1180\n",
       "20112012    1177\n",
       "20162017    1172\n",
       "20132014    1166\n",
       "20092010    1108\n",
       "20082009    1093\n",
       "20102011    1091\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ea672fed-3ac3-4725-9f2d-0422a91b243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20180226%10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9d562773-4685-4d5e-9143-97d97610169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9104, 27)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_no_early  = (X['mp_date'].apply(lambda x : x% 10**4) < 900) | (1100 < X['mp_date'].apply(lambda x : x% 10**4))\n",
    "\n",
    "X = X.loc[filt_no_early, : ].copy()  ## keep games < 900 and > 1100\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ec60470-3148-430c-b41d-387de6e33703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>game_id</th>\n",
       "      <th>mp_date</th>\n",
       "      <th>season</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_odds</th>\n",
       "      <th>away_odds</th>\n",
       "      <th>home_goals</th>\n",
       "      <th>away_goals</th>\n",
       "      <th>...</th>\n",
       "      <th>FSh%</th>\n",
       "      <th>FSv%</th>\n",
       "      <th>GDiff</th>\n",
       "      <th>GF%</th>\n",
       "      <th>PDO</th>\n",
       "      <th>PENDiff</th>\n",
       "      <th>SF%</th>\n",
       "      <th>SDiff</th>\n",
       "      <th>Sh%</th>\n",
       "      <th>Sv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5857</th>\n",
       "      <td>7467</td>\n",
       "      <td>2014020252</td>\n",
       "      <td>20141115</td>\n",
       "      <td>20142015</td>\n",
       "      <td>MTL</td>\n",
       "      <td>PHI</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.456462</td>\n",
       "      <td>0.568475</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-2.80</td>\n",
       "      <td>-1.03</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-1.785336</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5858</th>\n",
       "      <td>7468</td>\n",
       "      <td>2014020253</td>\n",
       "      <td>20141115</td>\n",
       "      <td>20142015</td>\n",
       "      <td>TBL</td>\n",
       "      <td>NYI</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.153596</td>\n",
       "      <td>0.225057</td>\n",
       "      <td>0.64</td>\n",
       "      <td>5.56</td>\n",
       "      <td>2.99</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>2.349725</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5859</th>\n",
       "      <td>7469</td>\n",
       "      <td>2014020254</td>\n",
       "      <td>20141115</td>\n",
       "      <td>20142015</td>\n",
       "      <td>NJD</td>\n",
       "      <td>COL</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.956409</td>\n",
       "      <td>-0.868548</td>\n",
       "      <td>0.43</td>\n",
       "      <td>4.32</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.373739</td>\n",
       "      <td>-0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>7470</td>\n",
       "      <td>2014020255</td>\n",
       "      <td>20141115</td>\n",
       "      <td>20142015</td>\n",
       "      <td>PIT</td>\n",
       "      <td>NYR</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.750878</td>\n",
       "      <td>1.476745</td>\n",
       "      <td>1.56</td>\n",
       "      <td>13.46</td>\n",
       "      <td>4.76</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.503294</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5861</th>\n",
       "      <td>7471</td>\n",
       "      <td>2014020256</td>\n",
       "      <td>20141115</td>\n",
       "      <td>20142015</td>\n",
       "      <td>CBJ</td>\n",
       "      <td>SJS</td>\n",
       "      <td>135.0</td>\n",
       "      <td>-155.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011364</td>\n",
       "      <td>-2.263427</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>-9.00</td>\n",
       "      <td>-3.05</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>-0.076481</td>\n",
       "      <td>-2.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>11768</td>\n",
       "      <td>2017020596</td>\n",
       "      <td>20171231</td>\n",
       "      <td>20172018</td>\n",
       "      <td>DAL</td>\n",
       "      <td>SJS</td>\n",
       "      <td>-137.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749470</td>\n",
       "      <td>-1.089864</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-2.59</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.706558</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>11769</td>\n",
       "      <td>2017020597</td>\n",
       "      <td>20171231</td>\n",
       "      <td>20172018</td>\n",
       "      <td>COL</td>\n",
       "      <td>NYI</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>-113.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.497050</td>\n",
       "      <td>0.501972</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.583425</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>11770</td>\n",
       "      <td>2017020598</td>\n",
       "      <td>20171231</td>\n",
       "      <td>20172018</td>\n",
       "      <td>ANA</td>\n",
       "      <td>ARI</td>\n",
       "      <td>-210.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930032</td>\n",
       "      <td>1.695756</td>\n",
       "      <td>1.03</td>\n",
       "      <td>8.95</td>\n",
       "      <td>4.09</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-2.24</td>\n",
       "      <td>-2.87</td>\n",
       "      <td>1.308561</td>\n",
       "      <td>2.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>11771</td>\n",
       "      <td>2017020599</td>\n",
       "      <td>20171231</td>\n",
       "      <td>20172018</td>\n",
       "      <td>CGY</td>\n",
       "      <td>CHI</td>\n",
       "      <td>-133.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641399</td>\n",
       "      <td>-0.284482</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-3.33</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.276535</td>\n",
       "      <td>-0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>11772</td>\n",
       "      <td>2017020600</td>\n",
       "      <td>20171231</td>\n",
       "      <td>20172018</td>\n",
       "      <td>EDM</td>\n",
       "      <td>WPG</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.312174</td>\n",
       "      <td>-1.257066</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-5.29</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>3.27</td>\n",
       "      <td>4.22</td>\n",
       "      <td>-1.691579</td>\n",
       "      <td>-1.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1495 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     game_id   mp_date    season home_team away_team  \\\n",
       "5857        7467  2014020252  20141115  20142015       MTL       PHI   \n",
       "5858        7468  2014020253  20141115  20142015       TBL       NYI   \n",
       "5859        7469  2014020254  20141115  20142015       NJD       COL   \n",
       "5860        7470  2014020255  20141115  20142015       PIT       NYR   \n",
       "5861        7471  2014020256  20141115  20142015       CBJ       SJS   \n",
       "...          ...         ...       ...       ...       ...       ...   \n",
       "9735       11768  2017020596  20171231  20172018       DAL       SJS   \n",
       "9736       11769  2017020597  20171231  20172018       COL       NYI   \n",
       "9737       11770  2017020598  20171231  20172018       ANA       ARI   \n",
       "9738       11771  2017020599  20171231  20172018       CGY       CHI   \n",
       "9739       11772  2017020600  20171231  20172018       EDM       WPG   \n",
       "\n",
       "      home_odds  away_odds  home_goals  away_goals  ...      FSh%      FSv%  \\\n",
       "5857     -160.0      140.0         6.0         3.0  ... -1.456462  0.568475   \n",
       "5858     -155.0      135.0         5.0         2.0  ...  2.153596  0.225057   \n",
       "5859     -110.0     -110.0         2.0         3.0  ...  0.956409 -0.868548   \n",
       "5860     -175.0      155.0         2.0         2.0  ...  1.750878  1.476745   \n",
       "5861      135.0     -155.0         2.0         1.0  ... -0.011364 -2.263427   \n",
       "...         ...        ...         ...         ...  ...       ...       ...   \n",
       "9735     -137.0      117.0         6.0         0.0  ...  0.749470 -1.089864   \n",
       "9736     -107.0     -113.0         6.0         1.0  ... -0.497050  0.501972   \n",
       "9737     -210.0      180.0         5.0         2.0  ...  0.930032  1.695756   \n",
       "9738     -133.0      113.0         4.0         3.0  ... -0.641399 -0.284482   \n",
       "9739     -143.0      123.0         0.0         5.0  ... -1.312174 -1.257066   \n",
       "\n",
       "      GDiff    GF%   PDO  PENDiff   SF%  SDiff       Sh%    Sv  \n",
       "5857  -0.30  -2.80 -1.03    -20.0 -0.23  -0.01 -1.785336  0.76  \n",
       "5858   0.64   5.56  2.99     -3.0 -1.68  -2.02  2.349725  0.64  \n",
       "5859   0.43   4.32  0.64     -3.0  2.43   3.21  1.373739 -0.74  \n",
       "5860   1.56  13.46  4.76      9.0  1.13   1.42  2.503294  2.26  \n",
       "5861  -1.11  -9.00 -3.05     -2.0 -1.36  -1.66 -0.076481 -2.97  \n",
       "...     ...    ...   ...      ...   ...    ...       ...   ...  \n",
       "9735  -0.26  -2.59 -0.80    -25.0 -0.34  -0.46  0.706558 -1.50  \n",
       "9736  -0.08  -0.62 -0.04      1.0 -0.59  -0.65 -0.583425  0.54  \n",
       "9737   1.03   8.95  4.09    -54.0 -2.24  -2.87  1.308561  2.77  \n",
       "9738  -0.37  -3.33 -1.04    -28.0 -0.33  -0.52 -0.276535 -0.76  \n",
       "9739  -0.65  -5.29 -3.26    -23.0  3.27   4.22 -1.691579 -1.56  \n",
       "\n",
       "[1495 rows x 26 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d21102ad-152a-4535-a93b-d2a7f72e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['Unnamed: 0', 'game_id', 'mp_date', 'season', 'home_team', 'away_team',\n",
    "       'home_odds', 'away_odds', 'home_goals', 'away_goals', 'goal_diff_target', 'home_win',\n",
    "       'settled_in', ]\n",
    "\n",
    "x_cols = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']\n",
    "columns_to_scale = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']  ##same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f4c9ae08-71a1-45e1-aeb8-4d710ee856a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = np.array(X.loc[(X['season'] <= 20152016), x_cols].copy())\n",
    "  #features test-train\n",
    "Y = X.loc[(X['season'] <= 20152016), y_cols].copy()\n",
    "   #targets\n",
    "y = np.array(Y['home_win']).reshape(-1,1)\n",
    "\n",
    "\n",
    "              \n",
    "x_16 = X.loc[(X['season'] == 20162017), x_cols].copy()  #features test-train\n",
    "Y_16 = X.loc[(X['season'] == 20162017), y_cols].copy()   #targets\n",
    "y_16 = np.array(Y_16['home_win']).reshape(-1,1)\n",
    "\n",
    "x_17 = X.loc[(X['season'] > 20162017), x_cols].copy()  #features test-train\n",
    "Y_17 = X.loc[(X['season'] > 20162017), y_cols].copy()   #targets\n",
    "y_17 = np.array(Y_17['home_win']).reshape(-1,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fc4a41e-da80-40f0-9e38-47f024dc9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for leakage?? 20172018 in the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "399f8bbc-785d-4d11-a5bd-2ee9bf4b5246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    407\n",
       "20162017    403\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] >= 20162017), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3749c2a0-fe5a-40a8-83d1-ac8e285d4d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20132014    408\n",
       "20112012    395\n",
       "20142015    388\n",
       "20152016    386\n",
       "20092010    381\n",
       "20102011    369\n",
       "20082009    358\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c56437cc-3da4-41c2-9b34-6f9146c5c9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2685"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f62af78-5775-422b-a01a-a0f4da5c7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3495, 26)\n",
      "(2685, 14) (2685, 12) (2685, 1)\n",
      "(403, 14) (403, 1)\n",
      "(407, 14) (407, 1)\n",
      "sum  2685 407  is:  3092\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(x.shape, Y.shape, y.shape)\n",
    "print(x_16.shape, y_16.shape)\n",
    "print(x_17.shape, y_17.shape)\n",
    "print('sum ', y.shape[0], y_17.shape[0], ' is: ', y.shape[0] + y_17.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ce6940c-1df9-4d1a-aeac-087fa6da4940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20132014    408\n",
       "20112012    395\n",
       "20142015    388\n",
       "20152016    386\n",
       "20092010    381\n",
       "20102011    369\n",
       "20082009    358\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5d07198-8efe-4156-8986-09e21000c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20162017    403\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_16['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75550a45-7737-4ea8-9184-1f433157700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    407\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_17['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9ecd118-f9dc-456b-8b1d-95f1e50d1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "std_scal = StandardScaler()\n",
    "mm_scal = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0df089-4313-488a-994f-1a628ad0117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e6d1c92-3ca5-447e-a9da-9393db3b9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38b8f56c-f0f0-4672-ac9c-50cdeed0696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7235643 ,  1.65934272,  1.90846329, ...,  0.02459198,\n",
       "         1.34759317,  1.61637573],\n",
       "       [ 0.49837896, -2.40556644,  0.58823629, ...,  0.25174124,\n",
       "        -2.23200983,  0.59322356],\n",
       "       [-0.53437454, -0.56969858,  0.41677738, ..., -0.58681814,\n",
       "        -0.29213671,  0.09476481],\n",
       "       ...,\n",
       "       [ 0.1222252 , -0.87257116, -1.65319514, ..., -0.37670507,\n",
       "        -0.5712479 , -1.52129093],\n",
       "       [ 0.57850638, -0.67386903, -0.34579725, ...,  0.75904128,\n",
       "        -0.43143098, -0.84443641],\n",
       "       [ 2.81539704,  2.68857895, -0.3907116 , ...,  2.58191417,\n",
       "         2.81847156, -0.37221233]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "65857f1c-005a-411c-b8d2-2f1430170d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRe  TEST error  0.5679702048417132 f1 0.6741573033707865\n",
      " training error  LogisticRe 0.5921787709497207 f1 0.6741573033707865\n",
      "SVC(random  TEST error  0.5642458100558659 f1 0.6794520547945204\n",
      " training error  SVC(random 0.6270949720670391 f1 0.6794520547945204\n",
      "[11:07:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassif  TEST error  0.5363128491620112 f1 0.6078740157480315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training error  XGBClassif 0.9981378026070763 f1 0.6078740157480315\n",
      "SVC(probab  TEST error  0.5642458100558659 f1 0.6794520547945204\n",
      " training error  SVC(probab 0.6270949720670391 f1 0.6794520547945204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgbq\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "clf_A = LogisticRegression(random_state = 2, max_iter = 1000)\n",
    "clf_B = SVC(random_state = 43, kernel = 'rbf')\n",
    "clf_C = xgb.XGBClassifier(seed = 44)\n",
    "clf_D = SVC(probability=True)\n",
    "\n",
    "for model in [clf_A, clf_B, clf_C, clf_D]: # [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "   \n",
    "    model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred = model.predict(x_test_sc)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    print(str(model)[0:10], ' TEST error ', acc, 'f1', f1)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(x_train_sc) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    print(' training error ', str(model)[0:10], acc_train_err, 'f1', f1) #f1_train_err)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2157c22-d2ff-454e-b3b7-f2bfeb9a11c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42)  TEST error on 20172018  0.6216216216216216 f1 0.738095238095238\n",
      "SVC(random_state=43)  TEST error on 20172018  0.6044226044226044 f1 0.7330016583747927\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)  TEST error on 20172018  0.5700245700245701 f1 0.6641074856046065\n",
      "SVC(probability=True)  TEST error on 20172018  0.6044226044226044 f1 0.7330016583747927\n"
     ]
    }
   ],
   "source": [
    "#Now check on X_18 \n",
    "#Now check on X_18 \n",
    "x_17_sc = std_scal.transform(x_17)\n",
    "x_test2 = x_17_sc.copy()\n",
    "y_test2 = y_17\n",
    "\n",
    "\n",
    "for model in [clf_A, clf_B, clf_C, clf_D]: # [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "   \n",
    "    #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred2 = model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred2)\n",
    "    f1 = f1_score(y_test2,y_pred2)\n",
    "    \n",
    "    print(model, ' TEST error on 20172018 ', acc, 'f1', f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "698d516c-a8ef-40c2-90e3-7bfed5f26276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42)  TEST error on 20162017  0.5930521091811415 f1 0.7028985507246377\n",
      "SVC(random_state=43)  TEST error on 20162017  0.5831265508684863 f1 0.6967509025270757\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)  TEST error on 20162017  0.5260545905707196 f1 0.6109979633401221\n",
      "SVC(probability=True)  TEST error on 20162017  0.5831265508684863 f1 0.6967509025270757\n"
     ]
    }
   ],
   "source": [
    "x#Now check on X_18 \n",
    "x_16_sc = std_scal.transform(x_16)\n",
    "x_test3 = x_16_sc.copy()\n",
    "y_test3 = y_16.copy()\n",
    "\n",
    "\n",
    "for model in [clf_A, clf_B, clf_C, clf_D]: # [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "   \n",
    "    #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred3 = model.predict(x_test3)\n",
    "  \n",
    "    acc = accuracy_score(y_test3, y_pred3)\n",
    "    f1 = f1_score(y_test3,y_pred3)\n",
    "    \n",
    "    print(model, ' TEST error on 20162017 ', acc, 'f1', f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9d55757-72f6-489f-a125-712cc1d856c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f42a35c1-bef7-43d6-8495-755ba5f893b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(random_state=42)  TEST error  0.5676117775354417 f1 0.6586310804993544\n",
      " training error  LogisticRegression(random_state=42) 0.5795717987181236 f1 0.6586310804993544\n",
      "SVC(random_state=43)  TEST error  0.5654307524536533 f1 0.6632868610054923\n",
      " training error  SVC(random_state=43) 0.598254466112096 f1 0.6632868610054923\n",
      "[15:50:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)  TEST error  0.5179934569247546 f1 0.5778414517669532\n",
      " training error  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None) 0.9451793263330152 f1 0.5778414517669532\n",
      "SVC(probability=True)  TEST error  0.5654307524536533 f1 0.6632868610054923\n",
      " training error  SVC(probability=True) 0.598254466112096 f1 0.6632868610054923\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgbq\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "clf_A = LogisticRegression(random_state = 42)\n",
    "clf_B = SVC(random_state = 43, kernel = 'rbf')\n",
    "clf_C = xgb.XGBClassifier(seed = 44)\n",
    "clf_D = SVC(probability=True)\n",
    "\n",
    "for model in [clf_A, clf_B, clf_C, clf_D]: # [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "   \n",
    "    model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred = model.predict(x_test_sc)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    print(str(model)[0:15], ' TEST error ', acc, 'f1', f1)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(x_train_sc) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    print(' training error ', model, acc_train_err, 'f1', f1) #f1_train_err)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e5883-765e-446a-8243-aee14bf56097",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now check on X_18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fe70075-480a-43f8-a07b-ba0de55e16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<module 'xgboost' from '/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/__init__.py'>\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a98fcad9-17ef-4eb4-aaa7-3be930bfce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10) TEST:  0.5623836126629422 0.6685472496473908 training :  0.6675977653631285 0.7473460721868366\n",
      "RidgeClassif TEST:  0.5642458100558659 0.6713483146067416 training :  0.5889199255121043 0.6867683575736077\n",
      "RandomForest TEST:  0.5363128491620112 0.6289120715350224 training :  0.9120111731843575 0.9263157894736842\n",
      "GaussianNB() TEST:  0.5381750465549349 0.5947712418300652 training :  0.5819366852886406 0.6310599835661462\n",
      "LogisticRegr TEST:  0.5716945996275605 0.6769662921348314 training :  0.5889199255121043 0.6858769121309143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "<ipython-input-193-f7cf8b62249a>:24: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(x_train_sc, y_train)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClass TEST:  0.5605214152700186 0.6277602523659306 training :  0.9990689013035382 0.9991714995857498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoos TEST:  0.5661080074487895 0.6666666666666666 training :  0.7337057728119181 0.792\n",
      "[20:47:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifie TEST:  0.5716945996275605 0.6676300578034682 training :  0.7863128491620112 0.8315596330275229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=314, stratify=y)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "##classifier models\n",
    "lrc = RidgeClassifier(alpha =0.2)\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0, C =10)\n",
    "svc = SVC(kernel = 'rbf', C =10)\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=10, random_state=0, n_estimators = 40)\n",
    "bc = BaggingClassifier(n_estimators  = 60, max_samples = 0.85)\n",
    "gbc = GradientBoostingClassifier(learning_rate =0.1, n_estimators = 40, max_depth =4 )\n",
    "xgbc = XGBClassifier(n_estimators= 45, eta=0.05)\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    model.fit(x_train_sc, y_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    acct = accuracy_score(y_train, y_predt)\n",
    "    f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5b0d4-02cb-4630-9274-163a821dca5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d136e63e-422c-48f7-9f45-293430a8fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10) TEST:  0.5823095823095823 0.7079037800687284\n",
      "RidgeClassif TEST:  0.6167076167076168 0.7346938775510204\n",
      "RandomForest TEST:  0.5700245700245701 0.6902654867256637\n",
      "GaussianNB() TEST:  0.547911547911548 0.6377952755905512\n",
      "LogisticRegr TEST:  0.6167076167076168 0.7346938775510204\n",
      "BaggingClass TEST:  0.5700245700245701 0.6601941747572815\n",
      "GradientBoos TEST:  0.597051597051597 0.7191780821917808\n",
      "XGBClassifie TEST:  0.5945945945945946 0.7140381282495667\n"
     ]
    }
   ],
   "source": [
    "x_17_sc = std_scal.transform(x_17)\n",
    "x_test2 = x_17_sc.copy()\n",
    "y_test2 = y_17.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7c84d576-3875-4725-937e-f8bd394bd515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10) TEST:  0.5707196029776674 0.6778398510242085 training :  0.595903165735568 0.7353658536585366\n",
      "RidgeClassif TEST:  0.5831265508684863 0.6956521739130435 training :  0.595903165735568 0.7353658536585366\n",
      "RandomForest TEST:  0.5607940446650124 0.6589595375722543 training :  0.595903165735568 0.7353658536585366\n",
      "GaussianNB() TEST:  0.5732009925558312 0.6371308016877637 training :  0.595903165735568 0.7353658536585366\n",
      "LogisticRegr TEST:  0.5905707196029777 0.6994535519125683 training :  0.595903165735568 0.7353658536585366\n",
      "BaggingClass TEST:  0.5359801488833746 0.6340508806262231 training :  0.595903165735568 0.7353658536585366\n",
      "GradientBoos TEST:  0.575682382133995 0.6815642458100559 training :  0.595903165735568 0.7353658536585366\n",
      "XGBClassifie TEST:  0.5558312655086849 0.6629001883239172 training :  0.595903165735568 0.7353658536585366\n"
     ]
    }
   ],
   "source": [
    "x_16_sc = std_scal.transform(x_16)\n",
    "x_test2 = x_16_sc.copy()\n",
    "y_test2 = y_16.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e100641-d559-4459-9087-a84e25ff7b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2a272036-7b29-4fd0-9f26-33076ad4fd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29148451 0.40520387 0.50665511 0.42079937 0.29841737]\n",
      "Ridge(alpha=50000) TEST:  0.6424581005586593 0.7818181818181819 training :  0.6303538175046555 0.7732724157624215\n",
      "[-0.23251353  0.71805064  1.09505305  0.52707299 -0.11915728]\n",
      "RandomForestRegresso TEST:  0.6182495344506518 0.7381864623243934 training :  0.6662011173184358 0.7654563297350344\n",
      "[-0.32802024  1.5485265   0.47738376  1.0301529   0.689174  ]\n",
      "XGBRegressor(base_sc TEST:  0.5698324022346368 0.6627737226277373 training :  0.9436685288640596 0.9533359043578866\n"
     ]
    }
   ],
   "source": [
    "##regression models now ...\n",
    "\n",
    "y = Y['goal_diff_target'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "lr = Ridge(alpha=50000) \n",
    "rfr = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [lr, rfr, xgbr]:\n",
    "    model.fit(x_train_sc, y_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    print(y_pred[0:5])\n",
    "    y_predw = v_win(y_pred)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    y_predwt = v_win(y_predt) \n",
    "    y_trainw = v_win(y_train)\n",
    "    y_testw = v_win(y_test)  #same as usual win/loss\n",
    "    acc = accuracy_score(y_testw, y_predw)\n",
    "    f1 = f1_score(y_testw,y_predw)\n",
    "    acct = accuracy_score(y_trainw, y_predwt)\n",
    "    f1t = f1_score(y_trainw,y_predwt)\n",
    "    \n",
    "  \n",
    "    print(str(model)[0:20], 'TEST: ', acc, f1 ,'training : ', acct, f1t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "91adc537-27c2-470b-8078-ad49432f474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_win(x):\n",
    "    if x>=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "v_win = np.vectorize(make_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "88eb8324-e05a-4d54-adc3-e68ccade60c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55317694 0.40599628 0.41538467 0.36832525 0.30777493]\n",
      "Ridge(alpha= TEST:  0.6203473945409429 0.7656967840735069\n",
      "[ 0.46260171  0.75274141  0.80154869  0.28970433 -0.58376368]\n",
      "RandomForest TEST:  0.6228287841191067 0.7379310344827585\n",
      "[-0.20716095 -2.5455654   0.30311152  1.9340824  -0.70554775]\n",
      "XGBRegressor TEST:  0.5707196029776674 0.6560636182902585\n"
     ]
    }
   ],
   "source": [
    "##regression models now ...\n",
    "x_test = x_16.copy()\n",
    "y_test = Y_16['goal_diff_target'].copy()\n",
    "\n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [lr, rfr, xgbr]:\n",
    "   \n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    print(y_pred[0:5])\n",
    "    y_predw = v_win(y_pred)\n",
    "   \n",
    "    y_testw = v_win(y_test)  #same as usual win/loss\n",
    "    \n",
    "    acc = accuracy_score(y_testw, y_predw)\n",
    "    f1 = f1_score(y_testw,y_predw)\n",
    "    \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "96f766be-37ec-409d-a0dd-0c09d0f371fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31938552 0.29756373 0.42518078 0.46607625 0.27714656]\n",
      "Ridge(alpha= TEST:  0.6167076167076168 0.7629179331306991\n",
      "[ 0.26194843  0.17763043  0.4769733   0.75905672 -0.70724261]\n",
      "RandomForest TEST:  0.601965601965602 0.7335526315789472\n",
      "[ 0.75880516  1.538813    1.0374336   2.035714   -0.8959621 ]\n",
      "XGBRegressor TEST:  0.5552825552825553 0.6552380952380952\n"
     ]
    }
   ],
   "source": [
    "##regression models now ...\n",
    "x_test = x_17.copy()\n",
    "y_test = Y_17['goal_diff_target'].copy()\n",
    "\n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [lr, rfr, xgbr]:\n",
    "   \n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    print(y_pred[0:5])\n",
    "    y_predw = v_win(y_pred)\n",
    "   \n",
    "    y_testw = v_win(y_test)  #same as usual win/loss\n",
    "    \n",
    "    acc = accuracy_score(y_testw, y_predw)\n",
    "    f1 = f1_score(y_testw,y_predw)\n",
    "    \n",
    "  \n",
    "    print(str(model)[0:12], 'TEST: ', acc, f1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e8efd-4cd3-464f-995c-6333d8a4279d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
