{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da468aa0-01bb-483e-b105-c2c46f980c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##part 1\n",
    "##use models with default\n",
    "##use data set with H/A +1, -1\n",
    "##do full window for now\n",
    "\n",
    "##next:\n",
    "##check if just excluding first 10 days helps (chaotic)\n",
    "##check if different windows help\n",
    "\n",
    "##next\n",
    "## can try tuning (for loops by hand, or ... use grid_search (use ML mastery code))\n",
    "##-I think tuning will be faster ... just do by hand ... loop over the possible things \n",
    "##-ONE for loop over i = (a,b,c,d)... for each model i[0]\n",
    "\n",
    "##Orrr can try adding features ... here we have to worry about:\n",
    "##-adding basic features eg pp, and correct fo%\n",
    "##-scaling numericals\n",
    "##-dummy vars for categoricals (are there any?) besides H/A\n",
    "##-num_windows and which lengths for moving avgs\n",
    "##-filtering the features for increasing complexity inteligently\n",
    "##-There is a dicotemy: \n",
    "##(a)use H/A + numerics or  ... here I think it can be made more like time-series\n",
    "##(b) just use mumerics (moving avg) ... here I think the order of the games is not important (note Leung did this, and random train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f72ab2-5c0f-487b-89dd-7182cd68b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc56975-d90e-4dba-8e73-abc8fab0a04b",
   "metadata": {},
   "source": [
    "\n",
    "##hyper_parameters from here \n",
    "##https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "##for xgboost from here \n",
    "##https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/\n",
    "\n",
    "#xgb\n",
    "\n",
    "trees = [10, 50, 100, 500, 1000, 5000]  #100  #num of trees\n",
    "max_depth = range(1,11)  ##3-5\n",
    "rates = [0.0001, 0.001, 0.01, 0.1, 1.0]  #0.1\n",
    "subsample in arange(0.1, 1.1, 0.1):  #0.4, 0.5  ##this is 0.1, 0.2 ... 1.0 # % of features to sample\n",
    "\n",
    "\n",
    "#svc \n",
    "kernels in [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’] #if you use poly, then adjust degree\n",
    "C in [100, 10, 1.0, 0.1, 0.001]\n",
    "\n",
    "#gb\n",
    "\n",
    "learning_rate in [0.001, 0.01, 0.1]\n",
    "n_estimators [10, 100, 1000]\n",
    "subsample in [0.5, 0.7, 1.0]\n",
    "max_depth in [3, 7, 9]\n",
    "\n",
    "\n",
    "#rfc\n",
    "max_features [1 to 20]  #key\n",
    "max_features in [‘sqrt’, ‘log2’]\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "#bc\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "svm_dic = {'kernels':[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’]}\n",
    "lrc_dic = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "lgr_hp_dic = {'solver': [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’], 'penalty' : [‘none’, ‘l1’, ‘l2’, ‘elasticnet’],\n",
    "'C' :[100, 10, 1.0, 0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef02f5ff-d836-4591-b5bd-6d4b61b0f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leung's features  WHYYYY is CSh% Csv% FSh% and Fsv% imporant?? that is so weird.\n",
    "##are these features the same as Cernak's?? Nope, same as Morrison! even calles it PENDiff \n",
    "#too .. maybe Morrison got it from somewhere too ... \n",
    "\n",
    "Leung_features = ['ID', \n",
    "               'Date', \n",
    "               'HomeTeam', \n",
    "               'AwayTeam', \n",
    "               'CF%', \n",
    "               'CSh%',\n",
    "               'CSv%',\n",
    "               'FF%',\n",
    "               'FSh%',\n",
    "               'FSv%',\n",
    "               'GDiff',\n",
    "               'GF%',\n",
    "               'PDO',\n",
    "               'PENDiff',\n",
    "               'SF%',\n",
    "               'SDiff',\n",
    "               'Sh%',\n",
    "               'Sv',\n",
    "               'FOW%',\n",
    "               'W%',\n",
    "               'FavoritesW%',\n",
    "               'Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ef210bf-f076-4e91-abdb-68bb6d42f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "##couple evaluation functions \n",
    "def evaluate_binary_classification(model_name, y_test, y_pred, y_proba=None, graph = False):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    #try:\n",
    "    if y_proba != None:\n",
    "        rocauc_score = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        rocauc_score = \"no roc\"\n",
    "    #except: \n",
    "    #    pass     \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    if graph == True:\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'{model_name}', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1 score: \", f1)\n",
    "    print(\"rocauc: \", rocauc_score)\n",
    "    print(cm)\n",
    "    #return accuracy, precision, recall, f1, rocauc_score\n",
    "\n",
    "def evaluate_regression(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"mae\", mae)\n",
    "    print(\"mse\", mse)\n",
    "    print('r2', r2)\n",
    "    \n",
    "##display null values\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee5ae41f-68fa-4db0-bbdb-88c6fb1b0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Data/Shaped_Data/data_bet_stats_mp.csv\")\n",
    "data.drop(columns=[ 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9235c761-bb3f-473c-a73d-86f9e157e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['won'] = data['won'].apply(int)\n",
    "data_playoffs = data.loc[data['playoffGame'] == 1, :].copy()  #set aside playoff games ... probably won't use them.\n",
    "data=  data.loc[data['playoffGame'] == 0, :].copy() \n",
    "\n",
    "#sorted(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176e3bfd-ced9-49b0-ae6a-5cf09464f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(X, season):  #season 20162017 int; returns the list of all dates in chronological order from that season 20161004, 20161005, ...\n",
    "    X = X.loc[X['season'] == season, :].copy()\n",
    "    dates_1 = list(set(X.loc[(X['date'] >= 900) & (X['date'] <= 1231) , :]['full_date']))  #2016 part\n",
    "    dates_2 = list(set(X.loc[(X['date'] >= 100) & (X['date'] <= 800) , :]['full_date']))  #2017 part\n",
    "    dates = dates_1 + dates_2 #all dates in order\n",
    "    return dates\n",
    "\n",
    "#sorted(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c4bffe-9d3a-441b-9951-ff4022a37b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_HA_data(X, season, list_var_names = None ):\n",
    "    X = X.loc[X['season'] == season, :].copy()\n",
    "    X_H = X.loc[X['HoA'] == 'home',:].copy()\n",
    "    X_A = X.loc[X['HoA'] == 'away',:].copy()\n",
    "    X_H['goal_difference'] = X_H['goalsFor'] - X_H['goalsAgainst']  ##note every thing is based in home data\n",
    "    X_H.reset_index(drop = True, inplace = True)\n",
    "    X_A.reset_index(drop = True, inplace = True)\n",
    "    df_visitor = pd.get_dummies(X_H['nhl_name'], dtype=np.int64)\n",
    "    df_home = pd.get_dummies(X_A['nhl_name'], dtype=np.int64)\n",
    "    df_model = df_home.sub(df_visitor) \n",
    "    df_model['date'] = X_H['date']\n",
    "    df_model['full_date'] = X_H['full_date']\n",
    "    \n",
    "    df_model['game_id'] = X_H['game_id']\n",
    "    df_model['home_id'] = X_H['team_id']\n",
    "    df_model['away_id'] = X_A['team_id'] \n",
    "    y = X_H.loc[:,['date', 'full_date','game_id', 'Open','goal_difference', 'won']].copy()   ##these are from home team perspective; 'Open' is for betting \n",
    "    return (df_model, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3598ea1f-e60b-46a8-a6da-a52e4c898438",
   "metadata": {},
   "outputs": [],
   "source": [
    "##have not written yet\n",
    "\n",
    "def top3_max_val_params(model, X, dates, drop_firs=False):\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d4647-c363-417c-95e2-f68b49533cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for  regressors predicting wins - losses, can use this to turn output into win prediction \n",
    "\n",
    "def make_win(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >0:\n",
    "        return 1\n",
    "\n",
    "v_make_win = np.vectorize(make_win)\n",
    "\n",
    "#useage: v_make_win(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa3db4c-8b79-45a7-9c7c-5682319ac106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_model_results(model, model_name, X, dates, step, window_size, prediction_size, drop_first_k_days = 0): #X = data \n",
    "    results_dic['model_name'] = []\n",
    "    results_dic['date'] = []\n",
    "    results_dic['mae'] = []\n",
    "    results_dic['mse'] = []\n",
    "    results_dic['r2'] = []   \n",
    "    \n",
    "    \n",
    "    #drop first k days from dates and X\n",
    "    dates = dates[drop_first_k_days :]\n",
    "    X = X.loc[X['full_date'].isin(dates), :].copy()  \n",
    "\n",
    "    for i in range(step, len(dates), step): ##eg step =10, so 17 rounds\n",
    "        \n",
    "        model.fit(X.loc[X['full_date'].isin(dates[max(i-window_size ,0):i]), :],y.loc[y['full_date'].isin(dates[max(i-window_size,0):i]),'goal_difference' ])\n",
    "        y_pred = model.predict(X.loc[X['full_date'].isin(dates[i:i+prediction_size]), :])\n",
    "        y_test = y.loc[y['full_date'].isin(dates[i:i+prediction_size]),'goal_difference' ]\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results_dic['model_name'].append(model_name)\n",
    "        results_dic['date'].append(dates[i])\n",
    "    \n",
    "        results_dic['mae'].append(mae)\n",
    "        results_dic['mse'].append(mse)\n",
    "        results_dic['r2'].append(r2)\n",
    "        \n",
    "    return results_dic #!\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c9989ff4-4152-488c-b97a-ca030013199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_model_results(model, model_name, X, dates, step, window_size, prediction_size, drop_first_k_days = 0): #X = data \n",
    "    results_dic ={}\n",
    "    results_dic['model_name'] = []\n",
    "    results_dic['date'] = []\n",
    "    results_dic['accuracy'] = []\n",
    "    results_dic['f1_score'] = []\n",
    "\n",
    "    #results_dic['precision'] = []\n",
    "  #  results_dic['recall'] = []\n",
    "    \n",
    "    #drop first k days from dates and X\n",
    "    dates = dates[drop_first_k_days :]\n",
    "    X = X.loc[X['full_date'].isin(dates), :].copy()  \n",
    "\n",
    "    for i in range(step, len(dates), step): ##eg step =10, so 17 rounds\n",
    "        model.fit(X.loc[X['full_date'].isin(dates[max(i-window_size ,0):i]), :],y.loc[y['full_date'].isin(dates[max(i-window_size,0):i]),'won' ])\n",
    "        y_pred = model.predict(X.loc[X['full_date'].isin(dates[i:i+prediction_size]), :])\n",
    "        y_test = y.loc[y['full_date'].isin(dates[i:i+prediction_size]),'won' ]\n",
    "    \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        #recision = precision_score(y_test, y_pred, zero_division = 0)\n",
    "        #recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred) #, average = None)\n",
    "            \n",
    "        results_dic['model_name'].append(model_name)  #append same model name every iter so same length as others\n",
    "        results_dic['date'].append(dates[i])\n",
    "                          \n",
    "        results_dic['accuracy'].append(accuracy)\n",
    "        results_dic['f1_score'].append(f1)\n",
    "        #results_dic['precision'].append(precision)\n",
    "        #results_dic['recall'].append(recall)\n",
    "    results_dic['model_name'].append('model_name'+'_avg')  #append same model name every iter so same length as others\n",
    "    results_dic['date'].append('average')\n",
    "    results_dic['accuracy'].append(round(np.mean(np.array(results_dic['accuracy'])), 2) ) \n",
    "    results_dic['f1_score'].append(round(np.mean(np.array(results_dic['f1_score'])), 2) ) \n",
    "    return results_dic #!\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ca466-1fa7-4229-8078-db5aed9cf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ok let's check the results of model lr = ridge(0.001) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dd847ff-1993-4bb6-ad29-4ace1738d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seasons = set(data['season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1028951-5fbc-487c-80f6-8263fe99c8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dic = {}\n",
    "y_dic = {}\n",
    "for sea in all_seasons:\n",
    "    X_dic[sea] = make_HA_data(data, sea)[0]\n",
    "    y_dic[sea] = make_HA_data(data, sea)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd302ced-9367-4921-b9c5-613d46f3da3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1      -3.0\n",
       "2       3.0\n",
       "3       1.0\n",
       "4      -3.0\n",
       "       ... \n",
       "1225    1.0\n",
       "1226    0.0\n",
       "1227   -2.0\n",
       "1228    1.0\n",
       "1229    3.0\n",
       "Name: goal_difference, Length: 1230, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dic[20162017].loc[:, 'goal_difference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "558fad63-c61e-4aac-afaa-baa29054925f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1271, 31)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dic[20172018].iloc[:, :-5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf1b2d8a-eb8d-471b-ac43-ae66cc6bc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "##naive method: train on first half of season, 600 games, test on second half of season\n",
    "##with no further training\n",
    "\n",
    "def naive_test_train(cut_off = 600):\n",
    "    for sea in all_seasons:\n",
    "        #select season, remove date, etc. select target y\n",
    "        y = y_dic[sea].loc[:, 'goal_difference'].copy()\n",
    "        X = X_dic[sea].iloc[:, :-5].copy()\n",
    "        \n",
    "        #carry out naive train-test split\n",
    "        y_train = y[0: cut_off].copy()\n",
    "        y_test = y[cut_off :].copy()\n",
    "        X_train = X[0: cut_off].copy()\n",
    "        X_test = X[cut_off :].copy()\n",
    "        \n",
    "        #train model, find predictions\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test) #this is regression pred on Hg - Ag\n",
    "        \n",
    "        y_pred_win = v_make_win(y_pred) #this is the pred of who wins HW =1, AW =0\n",
    "        y_test_win = v_make_win(y_test)  #this gives the correct win, loss\n",
    "        \n",
    "        print(\"seaoson: \", sea)\n",
    "        evaluate_regression(y_test, y_pred)\n",
    "        evaluate_binary_classification(y_test_win, y_pred_win)\n",
    "        print(\" \")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a254ba45-15e7-4f11-9efa-533954dff775",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ridge' object has no attribute 'pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6e5799bb79b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnaive_test_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcut_off\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-a13ffc004634>\u001b[0m in \u001b[0;36mnaive_test_train\u001b[0;34m(cut_off)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#train model, find predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this is regression pred on Hg - Ag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my_pred_win\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_make_win\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this is the pred of who wins HW =1, AW =0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Ridge' object has no attribute 'pred'"
     ]
    }
   ],
   "source": [
    "naive_test_train(cut_off = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476aa102-0a7d-4ba0-94d6-d762570f3c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
