{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b210964-63eb-4766-bac4-d91ee0dfbfe1",
   "metadata": {},
   "source": [
    "##I am splitting v3_Clean_model_add_Pis_feat.ipynb into 2 notebooks \n",
    "\n",
    "-this one on modelling and \n",
    "\n",
    "-another one on creating stats data set (just Pisch for now)\n",
    "v1_stats_tools_Pish.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a81d7-900b-46e1-9c38-4f1c9cdb3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "##part 1\n",
    "##use models with default\n",
    "##use data set with H/A +1, -1\n",
    "##do full window for now\n",
    "\n",
    "##next:\n",
    "##check if just excluding first 10 days helps (chaotic)\n",
    "##check if different windows help\n",
    "\n",
    "##next\n",
    "## can try tuning (for loops by hand, or ... use grid_search (use ML mastery code))\n",
    "##-I think tuning will be faster ... just do by hand ... loop over the possible things \n",
    "##-ONE for loop over i = (a,b,c,d)... for each model i[0]\n",
    "\n",
    "##Orrr can try adding features ... here we have to worry about:\n",
    "##-adding basic features eg pp, and correct fo%\n",
    "##-scaling numericals\n",
    "##-dummy vars for categoricals (are there any?) besides H/A\n",
    "##-num_windows and which lengths for moving avgs\n",
    "##-filtering the features for increasing complexity inteligently\n",
    "##-There is a dicotemy: \n",
    "##(a)use H/A + numerics or  ... here I think it can be made more like time-series\n",
    "##(b) just use mumerics (moving avg) ... here I think the order of the games is not important (note Leung did this, and random train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778fca46-90be-4c9f-a057-1e41ffea0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "##couple evaluation functions ##removed model_name as variable\n",
    "def evaluate_binary_classification(y_test, y_pred, y_proba=None, graph = False):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    #try:\n",
    "    if y_proba != None:\n",
    "        rocauc_score = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        rocauc_score = \"no roc\"\n",
    "    #except: \n",
    "    #    pass     \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    if graph == True:\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'{model_name}', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1 score: \", f1)\n",
    "    print(\"rocauc: \", rocauc_score)\n",
    "    print(cm)\n",
    "    #return accuracy, precision, recall, f1, rocauc_score\n",
    "\n",
    "def evaluate_regression(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"mae\", mae)\n",
    "    print(\"mse\", mse)\n",
    "    print('r2', r2)\n",
    "    \n",
    "##display null values\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d24d58a-39a7-44ed-95da-fbbeaaffbd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this takes the odds eg -200 is the favorite, 140 is underdog and says fav wins \n",
    "\n",
    "def fav_win(x):\n",
    "    if x <=0:\n",
    "        return 1\n",
    "    if x>0:\n",
    "        return 0\n",
    "    \n",
    "v_fav_win = np.vectorize(fav_win)\n",
    "\n",
    "\n",
    "def make_win(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >0:\n",
    "        return 1\n",
    "\n",
    "v_make_win = np.vectorize(make_win)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18af7514-a756-444a-bd2f-4de970ba321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_model_results(model, model_name, X, dates, step, window_size, prediction_size, drop_first_k_days = 0): #X = data \n",
    "    results_dic['model_name'] = []\n",
    "    results_dic['date'] = []\n",
    "    results_dic['mae'] = []\n",
    "    results_dic['mse'] = []\n",
    "    results_dic['r2'] = []   \n",
    "    \n",
    "    \n",
    "    #drop first k days from dates and X\n",
    "    dates = dates[drop_first_k_days :]\n",
    "    X = X.loc[X['full_date'].isin(dates), :].copy()  \n",
    "\n",
    "    for i in range(step, len(dates), step): ##eg step =10, so 17 rounds\n",
    "        \n",
    "        model.fit(X.loc[X['full_date'].isin(dates[max(i-window_size ,0):i]), :],y.loc[y['full_date'].isin(dates[max(i-window_size,0):i]),'goal_difference' ])\n",
    "        y_pred = model.predict(X.loc[X['full_date'].isin(dates[i:i+prediction_size]), :])\n",
    "        y_test = y.loc[y['full_date'].isin(dates[i:i+prediction_size]),'goal_difference' ]\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results_dic['model_name'].append(model_name)\n",
    "        results_dic['date'].append(dates[i])\n",
    "    \n",
    "        results_dic['mae'].append(mae)\n",
    "        results_dic['mse'].append(mse)\n",
    "        results_dic['r2'].append(r2)\n",
    "        \n",
    "    return results_dic #!\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7ec9a6-f01d-47c0-b2d1-02902750ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_model_results(model, model_name, X, dates, step, window_size, prediction_size, drop_first_k_days = 0): #X = data \n",
    "    results_dic ={}\n",
    "    results_dic['model_name'] = []\n",
    "    results_dic['date'] = []\n",
    "    results_dic['accuracy'] = []\n",
    "    results_dic['f1_score'] = []\n",
    "\n",
    "    #results_dic['precision'] = []\n",
    "  #  results_dic['recall'] = []\n",
    "    \n",
    "    #drop first k days from dates and X\n",
    "    dates = dates[drop_first_k_days :]\n",
    "    X = X.loc[X['full_date'].isin(dates), :].copy()  \n",
    "\n",
    "    for i in range(step, len(dates), step): ##eg step =10, so 17 rounds\n",
    "        model.fit(X.loc[X['full_date'].isin(dates[max(i-window_size ,0):i]), :],y.loc[y['full_date'].isin(dates[max(i-window_size,0):i]),'won' ])\n",
    "        y_pred = model.predict(X.loc[X['full_date'].isin(dates[i:i+prediction_size]), :])\n",
    "        y_test = y.loc[y['full_date'].isin(dates[i:i+prediction_size]),'won' ]\n",
    "    \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        #recision = precision_score(y_test, y_pred, zero_division = 0)\n",
    "        #recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred) #, average = None)\n",
    "            \n",
    "        results_dic['model_name'].append(model_name)  #append same model name every iter so same length as others\n",
    "        results_dic['date'].append(dates[i])\n",
    "                          \n",
    "        results_dic['accuracy'].append(accuracy)\n",
    "        results_dic['f1_score'].append(f1)\n",
    "        #results_dic['precision'].append(precision)\n",
    "        #results_dic['recall'].append(recall)\n",
    "    results_dic['model_name'].append('model_name'+'_avg')  #append same model name every iter so same length as others\n",
    "    results_dic['date'].append('average')\n",
    "    results_dic['accuracy'].append(round(np.mean(np.array(results_dic['accuracy'])), 2) ) \n",
    "    results_dic['f1_score'].append(round(np.mean(np.array(results_dic['f1_score'])), 2) ) \n",
    "    return results_dic #!\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c935eaa1-b629-406d-94f1-147c6daed626",
   "metadata": {},
   "outputs": [],
   "source": [
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cbaf7e-46af-4a63-9a9c-bbe35291577d",
   "metadata": {},
   "source": [
    "##TUNING INFO \n",
    "\n",
    "\n",
    "##hyper_parameters from here \n",
    "##https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "##for xgboost from here \n",
    "##https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/\n",
    "\n",
    "#xgb\n",
    "\n",
    "trees = [10, 50, 100, 500, 1000, 5000]  #100  #num of trees\n",
    "max_depth = range(1,11)  ##3-5\n",
    "rates = [0.0001, 0.001, 0.01, 0.1, 1.0]  #0.1\n",
    "subsample in arange(0.1, 1.1, 0.1):  #0.4, 0.5  ##this is 0.1, 0.2 ... 1.0 # % of features to sample\n",
    "\n",
    "\n",
    "#svc \n",
    "kernels in [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’] #if you use poly, then adjust degree\n",
    "C in [100, 10, 1.0, 0.1, 0.001]\n",
    "\n",
    "#gb\n",
    "\n",
    "learning_rate in [0.001, 0.01, 0.1]\n",
    "n_estimators [10, 100, 1000]\n",
    "subsample in [0.5, 0.7, 1.0]\n",
    "max_depth in [3, 7, 9]\n",
    "\n",
    "\n",
    "#rfc\n",
    "max_features [1 to 20]  #key\n",
    "max_features in [‘sqrt’, ‘log2’]\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "#bc\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "svm_dic = {'kernels':[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’]}\n",
    "lrc_dic = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "lgr_hp_dic = {'solver': [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’], 'penalty' : [‘none’, ‘l1’, ‘l2’, ‘elasticnet’],\n",
    "'C' :[100, 10, 1.0, 0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbb7207-4cc5-47b0-aa6b-7ba80d933695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC(kernel = 'rbf')\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=5, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b36d368b-45fc-44c1-a5ff-579fea4fe286",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic = {}\n",
    "\n",
    "file_path_12 = '/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Note_books/Explore_Models/data_dummies_Pis_v2_20122013.csv'\n",
    "data_dic[20122013] = pd.read_csv(file_path_12)\n",
    "\n",
    "for season in [20152016, 20162017, 20172018, 20182019]:   \n",
    "    \n",
    "    file_path_seas = '/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Note_books/Explore_Models/'+'data_dummies_Pis_xg_Corsi_v3_'+str(season)+'.csv'\n",
    "    data_dic[season] = pd.read_csv(file_path_seas)\n",
    " # data_bootcamp/GitHub/final_project_nhl_prediction/Note_books/Explore_Models/data_dummies_Pis_v2_20122013.csv\n",
    "#data_bootcamp/GitHub/final_project_nhl_prediction/Note_books/Explore_Models/data_dummies_Pis_xg_Corsi_v3_20152016.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ddc4a4-28f5-4a50-a16a-5fccec52b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "k =5\n",
    "for season in [20122013,20152016, 20162017, 20172018, 20182019]:    \n",
    "    filter = (data_dic[season]['full_date'] >= data_dic[season]['full_date'][0]+k).copy() #removes first k = 5 days of season where there are nan values \n",
    "    data_dic[season]= data_dic[season].loc[filter, :].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38fe6f08-f93a-4e4d-a225-f4e4cf4160d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_12 = data_dic[20122013].copy()  \n",
    "data_15 = data_dic[20152016].copy()\n",
    "data_16  = data_dic[20162017].copy()\n",
    "data_17 = data_dic[20172018].copy()\n",
    "data_18 = data_dic[20182019].copy()\n",
    "data_15_17 = pd.concat([data_15,data_16])\n",
    "data_17_19 = pd.concat([data_17,data_18])\n",
    "#Note Bene\n",
    "data_12.rename(columns ={'win%':'win%_cumul'}, inplace = True)\n",
    "data_12.rename(columns ={'last_10_games_win%' :'win%_last_10_games'}, inplace = True)\n",
    "#(1230, 50)\n",
    "#(1230, 50)\n",
    "#(1271, 51) Vegas, baby\n",
    "#(1271, 51)'win%_last_10_games'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e289286b-800d-4871-9e70-8d23a6373f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perc_null(data_18) takes 5 days to get rid of NaN ... also good to get rid of randomness in first 5 days (maybe even 20 days ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cf071-2df8-41a3-8d37-cb37b9e3053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_12.iloc[:10,7:] \n",
    "\n",
    "##columns are all safe ... \n",
    "#win%_cumul HAS to be previous day (NOT including day of ... o/w model can inspect \n",
    "#which teams win% went up and which ... actually kinda tough bec it's difference )\n",
    "##anyway I checked in v1_stats 'SJS' ... that win% = win%_cumul is *strictly* the previous days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d3ad79d-3202-44d4-8436-b6523d347b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['goalsAgainst_cumul_sum', 'goalsFor_cumul_sum', 'goalsDiff_cumul_sum',\n",
       "       'goalsFor%_cumul_avg', 'pp%_cumul_avg', 'pk%_cumul_avg',\n",
       "       'sh%_cumul_avg', 'sv%_cumul_avg', 'PDO_cumul_avg',\n",
       "       'fenwickPercentage_cumul_avg', 'win%_last_10_games', 'win%_cumul'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_scale = list(data_15.iloc[:5, 38:].columns)\n",
    "data_15.iloc[:5, 38:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeea4a3b-2293-4484-8239-1294f6228e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'game_id', 'full_date', 'home_team', 'away_team', 'Open',\n",
       "       'goal_difference', 'won'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_target = list(data_15.iloc[:5, :8].columns)\n",
    "data_15.iloc[:5, :8].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8826372f-b7a0-40c7-ac29-0416de7e4017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_12 = data_12.iloc[:,7:].copy() \n",
    "X_15 = data_15.drop(columns = columns_target ).copy()\n",
    "X_16 = data_16.drop(columns = columns_target ).copy()\n",
    "X_17 = data_17.drop(columns = columns_target ).copy()\n",
    "X_18 = data_18.drop(columns = columns_target ).copy()\n",
    "y_12 = data_12.iloc[:,:7].copy() \n",
    "y_15 = data_15.loc[:, columns_target ].copy()\n",
    "y_16 = data_16.loc[:, columns_target ].copy()\n",
    "y_17 = data_17.loc[:, columns_target ].copy()\n",
    "y_18 = data_18.loc[:, columns_target ].copy()\n",
    "list_X = [X_12, X_15,X_16, X_17, X_18]\n",
    "list_y = [y_12, y_15, y_16,y_17, y_18 ]\n",
    "list_Xy = zip(list_X, list_y)\n",
    "list_data = [data_12, data_15, data_16, data_17, data_18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76397d24-e8e9-4811-b795-a81108bf5bb6",
   "metadata": {},
   "source": [
    "End of baselines ... on to modelling ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "53f2cc86-387d-42a1-8f0d-7299a35c8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "sgdr = SGDRegressor() #has partial_fit()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()  #has partial_fit()\n",
    "lgr = LogisticRegression(random_state = 0, max_iter = 10**5)\n",
    "svc = SVC()\n",
    "sgdc = SGDClassifier() #has partial_fit()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "##have partial_fit()\n",
    "\n",
    "\n",
    "#['BernoulliNB', 'GaussianNB', 'MiniBatchKMeans', 'MultinomialNB', 'PassiveAggressiveClassifier', PassiveAggressiveRegressor', 'Perceptron', 'SGDClassifier', 'SGDRegressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "586a5515-3122-4f45-898d-4aa144910e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "std_scal = StandardScaler()\n",
    "mm_scal = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2af84e96-cafa-4f33-90e6-418835eefcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1199, 42)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_15 as test case\n",
    "X_15.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bd8006f-6434-4879-9d14-6ce3857e69de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['goalsAgainst_cumul_sum', 'goalsFor_cumul_sum', 'goalsDiff_cumul_sum',\n",
       "       'goalsFor%_cumul_avg', 'pp%_cumul_avg', 'pk%_cumul_avg',\n",
       "       'sh%_cumul_avg', 'sv%_cumul_avg', 'PDO_cumul_avg',\n",
       "       'fenwickPercentage_cumul_avg', 'win%_last_10_games', 'win%_cumul'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_scale = list(data_15.iloc[:5, 38:].columns)\n",
    "data_15.iloc[:5, 38:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "493ae5f3-f2fe-448e-ae56-81819dd75f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'game_id', 'full_date', 'home_team', 'away_team', 'Open',\n",
       "       'goal_difference', 'won'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_target = list(data_15.iloc[:5, :8].columns)\n",
    "data_15.iloc[:5, :8].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20b1bc93-8467-4b8c-bbc3-ec1a07dfc470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimenting with std_scaler ... \n",
    "#NOte Bene ... you must assign the numpy object std_scal.fit_transform(Y.loc[:,columns_to_scale]).copy()\n",
    "#to the df ... if you assign pd.DataFrame(same) it will give NaNs (why?)\n",
    " \n",
    "Y = X_12.iloc[:300, :].copy()\n",
    "#Y.loc[:,columns_to_scale] = pd.DataFrame(std_scal.fit_transform(Y.loc[:,columns_to_scale])).copy()\n",
    "Y.loc[:,columns_to_scale] = std_scal.fit_transform(Y.loc[:,columns_to_scale]).copy()\n",
    "Y.loc[:,columns_to_scale]\n",
    "Y2 = X_12.iloc[300:, :].copy()\n",
    "Y2.loc[:,columns_to_scale] = std_scal.transform(Y2.loc[:,columns_to_scale])\n",
    "#X_12.iloc[300:, :]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bd82c48-3847-41c9-a67b-08d1e2406e82",
   "metadata": {},
   "source": [
    "str(lrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4efe1314-e15e-415a-bb5a-d0bde3acf099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier()  TEST error  0.51\n"
     ]
    }
   ],
   "source": [
    "T = 800  #train until this game in season (out of 1200)\n",
    "d = 100  #predict this many games \n",
    "scal = std_scal\n",
    "#X_15 has about 1200 games\n",
    "\n",
    "#set the data frame and target\n",
    "X = X_15.copy()\n",
    "y = y_15.loc[:, 'won'].copy()\n",
    "\n",
    "#how does it predict on next season?? Terrible! lol ... train around 80% test 47%\n",
    "#W = X_16.copy()\n",
    "#z = y_16.loc[:, 'won'].copy()\n",
    "\n",
    "for model in [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "    y_train= y.iloc[:T].copy()\n",
    "    y_test = y.iloc[T:T+d].copy()\n",
    "    #y_test = z.iloc[:100].copy()\n",
    "\n",
    "    X_train = X.iloc[:T, :].copy()\n",
    "    X_test= X.iloc[T:T+d, :].copy()   \n",
    "    #X_test = W.iloc[:100].copy()\n",
    "    \n",
    "    #do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "    #X_train.loc[:, columns_to_scale] = scal.fit_transform(X_train.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    #fit the scaler from train portion to the test portion \n",
    "    #X_test.loc[:, columns_to_scale] = scal.transform(X_test.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    print(model, ' TEST error ', acc,)# f1)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(X_train) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    #print(' training error ', model, acc_train_err, ) #f1_train_err)\n",
    "   \n",
    "   \n",
    "#I am following this stackexchange\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#In [93]: mms = MinMaxScaler()\n",
    "#In [94]: df[['x','z']] = mms.fit_transform(df[['x','z']])\n",
    "#the one with check mark does pipe line tho :-) https://stackoverflow.com/questions/43834242\n",
    "\n",
    "\n",
    "#ugghhh! terrible ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fee00-a8f0-44ce-a6d6-ca1d06531e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok pretty pretty good start ... logistic and ridgec 0.5835. (they are identical .. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95e803-77c9-47f3-a160-4bb2b5787882",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4df6ad2b-c29e-4af4-a25c-6fdb32746a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()  train error  0.95  f1_train  0.9411764705882353  TEST ERROR  0.5  f1  0.5454545454545454\n",
      "GaussianNB()  train error  0.75  f1_train  0.782608695652174  TEST ERROR  0.55  f1  0.5714285714285713\n",
      "GaussianNB()  train error  0.8833333333333333  f1_train  0.8727272727272727  TEST ERROR  0.6  f1  0.6\n",
      "GaussianNB()  train error  0.8125  f1_train  0.8235294117647058  TEST ERROR  0.6  f1  0.6666666666666666\n",
      "GaussianNB()  train error  0.84  f1_train  0.8367346938775511  TEST ERROR  0.5  f1  0.5454545454545455\n",
      "GaussianNB()  train error  0.6666666666666666  f1_train  0.5348837209302326  TEST ERROR  0.65  f1  0.46153846153846156\n",
      "GaussianNB()  train error  0.7  f1_train  0.58  TEST ERROR  0.4  f1  0.14285714285714288\n",
      "GaussianNB()  train error  0.725  f1_train  0.6986301369863013  TEST ERROR  0.5  f1  0.5\n",
      "GaussianNB()  train error  0.7  f1_train  0.6707317073170732  TEST ERROR  0.55  f1  0.5263157894736842\n",
      "GaussianNB()  train error  0.715  f1_train  0.6779661016949152  TEST ERROR  0.35  f1  0.3157894736842105\n",
      "GaussianNB()  train error  0.6772727272727272  f1_train  0.64321608040201  TEST ERROR  0.55  f1  0.5714285714285715\n",
      "GaussianNB()  train error  0.6541666666666667  f1_train  0.6244343891402716  TEST ERROR  0.45  f1  0.4761904761904762\n",
      "GaussianNB()  train error  0.6538461538461539  f1_train  0.6428571428571429  TEST ERROR  0.6  f1  0.6363636363636364\n",
      "GaussianNB()  train error  0.6392857142857142  f1_train  0.6273062730627307  TEST ERROR  0.55  f1  0.5263157894736842\n",
      "GaussianNB()  train error  0.6433333333333333  f1_train  0.6445182724252492  TEST ERROR  0.25  f1  0.34782608695652173\n",
      "GaussianNB()  train error  0.615625  f1_train  0.6215384615384615  TEST ERROR  0.75  f1  0.8148148148148148\n",
      "GaussianNB()  train error  0.638235294117647  f1_train  0.6554621848739497  TEST ERROR  0.55  f1  0.5714285714285713\n",
      "GaussianNB()  train error  0.6611111111111111  f1_train  0.6755319148936171  TEST ERROR  0.7  f1  0.7500000000000001\n",
      "GaussianNB()  train error  0.6631578947368421  f1_train  0.678391959798995  TEST ERROR  0.6  f1  0.6923076923076923\n",
      "GaussianNB()  train error  0.6675  f1_train  0.6914153132250581  TEST ERROR  0.65  f1  0.631578947368421\n",
      "GaussianNB()  train error  0.6547619047619048  f1_train  0.6697038724373576  TEST ERROR  0.45  f1  0.47619047619047616\n",
      "GaussianNB()  train error  0.6590909090909091  f1_train  0.6710526315789473  TEST ERROR  0.65  f1  0.7199999999999999\n",
      "GaussianNB()  train error  0.6565217391304348  f1_train  0.6708333333333334  TEST ERROR  0.55  f1  0.608695652173913\n",
      "GaussianNB()  train error  0.6479166666666667  f1_train  0.674373795761079  TEST ERROR  0.45  f1  0.5599999999999999\n",
      "GaussianNB()  train error  0.652  f1_train  0.6753731343283581  TEST ERROR  0.5  f1  0.5833333333333334\n",
      "GaussianNB()  train error  0.6461538461538462  f1_train  0.6702508960573478  TEST ERROR  0.55  f1  0.5714285714285715\n",
      "GaussianNB()  train error  0.6425925925925926  f1_train  0.6666666666666667  TEST ERROR  0.45  f1  0.56\n",
      "GaussianNB()  train error  0.6321428571428571  f1_train  0.6578073089700998  TEST ERROR  0.45  f1  0.5217391304347826\n",
      "GaussianNB()  train error  0.6379310344827587  f1_train  0.670846394984326  TEST ERROR  0.45  f1  0.5217391304347826\n",
      "GaussianNB()  train error  0.6183333333333333  f1_train  0.6427457098283932  TEST ERROR  0.6  f1  0.6666666666666667\n",
      "GaussianNB()  train error  0.6241935483870967  f1_train  0.6517189835575484  TEST ERROR  0.3  f1  0.3636363636363636\n",
      "GaussianNB()  train error  0.6109375  f1_train  0.6406926406926406  TEST ERROR  0.65  f1  0.631578947368421\n",
      "GaussianNB()  train error  0.6166666666666667  f1_train  0.6421499292786421  TEST ERROR  0.45  f1  0.47619047619047616\n",
      "GaussianNB()  train error  0.6205882352941177  f1_train  0.6446280991735537  TEST ERROR  0.6  f1  0.6363636363636365\n",
      "GaussianNB()  train error  0.6185714285714285  f1_train  0.642570281124498  TEST ERROR  0.5  f1  0.5833333333333334\n",
      "GaussianNB()  train error  0.6166666666666667  f1_train  0.6424870466321243  TEST ERROR  0.4  f1  0.4545454545454546\n",
      "GaussianNB()  train error  0.6162162162162163  f1_train  0.6395939086294417  TEST ERROR  0.55  f1  0.6666666666666666\n",
      "GaussianNB()  train error  0.6118421052631579  f1_train  0.6398046398046399  TEST ERROR  0.7  f1  0.6666666666666665\n",
      "GaussianNB()  train error  0.6192307692307693  f1_train  0.6468489892984542  TEST ERROR  0.5  f1  0.5454545454545454\n",
      "GaussianNB()  train error  0.61625  f1_train  0.6426076833527357  TEST ERROR  0.5  f1  0.5833333333333334\n",
      "GaussianNB()  train error  0.6207317073170732  f1_train  0.649379932356257  TEST ERROR  0.4  f1  0.45454545454545453\n",
      "GaussianNB()  train error  0.6130952380952381  f1_train  0.6455834242093784  TEST ERROR  0.7  f1  0.7272727272727272\n",
      "GaussianNB()  train error  0.6174418604651163  f1_train  0.6458557588805166  TEST ERROR  0.3  f1  0.41666666666666663\n",
      "GaussianNB()  train error  0.6102272727272727  f1_train  0.644559585492228  TEST ERROR  0.7  f1  0.7272727272727273\n",
      "GaussianNB()  train error  0.6166666666666667  f1_train  0.6525679758308158  TEST ERROR  0.55  f1  0.6666666666666665\n",
      "GaussianNB()  train error  0.6184782608695653  f1_train  0.6500498504486539  TEST ERROR  0.45  f1  0.5217391304347827\n",
      "GaussianNB()  train error  0.624468085106383  f1_train  0.6569484936831875  TEST ERROR  0.6  f1  0.6\n",
      "GaussianNB()  train error  0.6197916666666666  f1_train  0.6520495710200189  TEST ERROR  0.8  f1  0.8461538461538461\n",
      "GaussianNB()  train error  0.6224489795918368  f1_train  0.6593001841620626  TEST ERROR  0.35  f1  0.43478260869565216\n",
      "GaussianNB()  train error  0.615  f1_train  0.6522131887985546  TEST ERROR  0.4  f1  0.5384615384615385\n",
      "GaussianNB()  train error  0.6078431372549019  f1_train  0.6472663139329807  TEST ERROR  0.5  f1  0.6428571428571429\n",
      "GaussianNB()  train error  0.6105769230769231  f1_train  0.6505608283002589  TEST ERROR  0.45  f1  0.5217391304347826\n",
      "GaussianNB()  train error  0.6132075471698113  f1_train  0.6537162162162163  TEST ERROR  0.65  f1  0.6956521739130435\n",
      "GaussianNB()  train error  0.6083333333333333  f1_train  0.6489626556016598  TEST ERROR  0.45  f1  0.4761904761904762\n",
      "GaussianNB()  train error  0.5981818181818181  f1_train  0.640650406504065  TEST ERROR  0.55  f1  0.6666666666666666\n",
      "GaussianNB()  train error  0.5991071428571428  f1_train  0.6444972288202693  TEST ERROR  0.55  f1  0.64\n",
      "GaussianNB()  train error  0.5956140350877193  f1_train  0.6434648105181748  TEST ERROR  0.6  f1  0.6363636363636364\n",
      "GaussianNB() NOO partial_fit avg train error  0.6541378173594579  AVERAGE TEST ERROR  0.5280701754385964\n"
     ]
    }
   ],
   "source": [
    "#T = 800  #train until this game in season (out of 1200)\n",
    "d = 20  #predict this many games \n",
    "scal = std_scal\n",
    "#X_15 has about 1200 games\n",
    "\n",
    "#set the data frame and target\n",
    "X = X_15.copy()\n",
    "y = y_15.loc[:, 'won'].copy()\n",
    "\n",
    "model = gnb\n",
    "#model = sgdc\n",
    "##quick checks \n",
    "\n",
    "counter = 0\n",
    "acc_sum_train = 0\n",
    "acc_sum_test = 0\n",
    "#for model in [gnb, sgdc]:  #partial_fit\n",
    "for T in range(d, 1160,d):\n",
    "    y_train= y.iloc[:T].copy()\n",
    "    y_test = y.iloc[T:T+d].copy()\n",
    "    #y_test = z.iloc[:100].copy()\n",
    "\n",
    "    X_train = X.iloc[:T, :].copy()\n",
    "    X_test= X.iloc[T:T+d, :].copy()   \n",
    "    #X_test = W.iloc[:100].copy()\n",
    "    \n",
    "    #do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "    X_train.loc[:, columns_to_scale] = scal.fit_transform(X_train.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    #fit the scaler from train portion to the test portion \n",
    "    X_test.loc[:, columns_to_scale] = scal.transform(X_test.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(X_train) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    #print(' training error ', model, acc_train_err, ) #f1_train_err)\n",
    "   \n",
    "    \n",
    "    print(model, ' train error ', acc_train_err, ' f1_train ', f1_train_err, ' TEST ERROR ', acc, ' f1 ', f1)\n",
    "    acc_sum_test += acc\n",
    "    acc_sum_train += acc_train_err\n",
    "    counter +=1\n",
    "avg_acc = acc_sum_test/counter\n",
    "avg_acc_train = acc_sum_train/counter\n",
    "print(model, 'NOO partial_fit' ' avg train error ', avg_acc_train, ' AVERAGE TEST ERROR ', avg_acc,)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5fc7b94f-cf6f-4b49-81a8-afb8d3c68f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6  TEST ERROR  0.75  f1  0.7826086956521738\n",
      "GaussianNB() with partial_fit   train error  0.8  f1_train  0.8181818181818182  TEST ERROR  0.55  f1  0.608695652173913\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.7272727272727272  TEST ERROR  0.65  f1  0.6956521739130435\n",
      "GaussianNB() with partial_fit   train error  0.8  f1_train  0.8181818181818181  TEST ERROR  0.5  f1  0.5454545454545454\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.6666666666666665  TEST ERROR  0.55  f1  0.5714285714285714\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6956521739130435  TEST ERROR  0.5  f1  0.4444444444444444\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6  TEST ERROR  0.55  f1  0.6086956521739131\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6363636363636364  TEST ERROR  0.65  f1  0.588235294117647\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.7368421052631577  TEST ERROR  0.65  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.7368421052631577  TEST ERROR  0.4  f1  0.5\n",
      "GaussianNB() with partial_fit   train error  0.5  f1_train  0.5454545454545454  TEST ERROR  0.6  f1  0.6923076923076924\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.7199999999999999  TEST ERROR  0.45  f1  0.4761904761904762\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.6086956521739131  TEST ERROR  0.65  f1  0.6956521739130435\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.8  TEST ERROR  0.7  f1  0.7000000000000001\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.7368421052631577  TEST ERROR  0.35  f1  0.4347826086956522\n",
      "GaussianNB() with partial_fit   train error  0.5  f1_train  0.5454545454545454  TEST ERROR  0.55  f1  0.6086956521739131\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.75  TEST ERROR  0.6  f1  0.6363636363636364\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6666666666666666  TEST ERROR  0.55  f1  0.608695652173913\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6363636363636364  TEST ERROR  0.65  f1  0.7407407407407406\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.7407407407407406  TEST ERROR  0.7  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.6666666666666666  TEST ERROR  0.65  f1  0.631578947368421\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.7  TEST ERROR  0.55  f1  0.5714285714285715\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6956521739130435  TEST ERROR  0.55  f1  0.608695652173913\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.608695652173913  TEST ERROR  0.5  f1  0.5833333333333334\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.64  TEST ERROR  0.55  f1  0.6086956521739131\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6666666666666667  TEST ERROR  0.6  f1  0.6\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.631578947368421  TEST ERROR  0.5  f1  0.5454545454545454\n",
      "GaussianNB() with partial_fit   train error  0.5  f1_train  0.5454545454545454  TEST ERROR  0.6  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6956521739130435  TEST ERROR  0.35  f1  0.380952380952381\n",
      "GaussianNB() with partial_fit   train error  0.4  f1_train  0.39999999999999997  TEST ERROR  0.6  f1  0.6666666666666667\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.7826086956521738  TEST ERROR  0.35  f1  0.380952380952381\n",
      "GaussianNB() with partial_fit   train error  0.35  f1_train  0.380952380952381  TEST ERROR  0.7  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.7368421052631577  TEST ERROR  0.55  f1  0.5714285714285713\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6363636363636365  TEST ERROR  0.6  f1  0.6\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6666666666666666  TEST ERROR  0.65  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.7272727272727274  TEST ERROR  0.45  f1  0.4761904761904762\n",
      "GaussianNB() with partial_fit   train error  0.5  f1_train  0.5454545454545455  TEST ERROR  0.55  f1  0.64\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.7407407407407408  TEST ERROR  0.75  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.8  f1_train  0.75  TEST ERROR  0.65  f1  0.631578947368421\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.631578947368421  TEST ERROR  0.5  f1  0.5\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6363636363636365  TEST ERROR  0.4  f1  0.45454545454545453\n",
      "GaussianNB() with partial_fit   train error  0.5  f1_train  0.5833333333333334  TEST ERROR  0.7  f1  0.7\n",
      "GaussianNB() with partial_fit   train error  0.8  f1_train  0.8000000000000002  TEST ERROR  0.35  f1  0.4799999999999999\n",
      "GaussianNB() with partial_fit   train error  0.4  f1_train  0.4999999999999999  TEST ERROR  0.8  f1  0.8181818181818182\n",
      "GaussianNB() with partial_fit   train error  0.8  f1_train  0.8181818181818182  TEST ERROR  0.65  f1  0.6956521739130435\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6956521739130435  TEST ERROR  0.5  f1  0.5\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6  TEST ERROR  0.65  f1  0.588235294117647\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.6666666666666665  TEST ERROR  0.85  f1  0.8799999999999999\n",
      "GaussianNB() with partial_fit   train error  0.85  f1_train  0.8799999999999999  TEST ERROR  0.35  f1  0.380952380952381\n",
      "GaussianNB() with partial_fit   train error  0.4  f1_train  0.39999999999999997  TEST ERROR  0.5  f1  0.5833333333333334\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.64  TEST ERROR  0.5  f1  0.6153846153846153\n",
      "GaussianNB() with partial_fit   train error  0.5  f1_train  0.5833333333333334  TEST ERROR  0.45  f1  0.47619047619047616\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.6086956521739131  TEST ERROR  0.6  f1  0.6363636363636365\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6363636363636365  TEST ERROR  0.5  f1  0.4444444444444445\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.5555555555555556  TEST ERROR  0.55  f1  0.64\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6666666666666666  TEST ERROR  0.55  f1  0.6086956521739131\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.6086956521739131  TEST ERROR  0.65  f1  0.631578947368421\n",
      "GaussianNB() with partial_fit   avg train error  0.6289473684210525  AVERAGE TEST ERROR  0.5666666666666669\n"
     ]
    }
   ],
   "source": [
    "#T = 800  #train until this game in season (out of 1200)\n",
    "d = 20  #predict this many games \n",
    "scal = std_scal\n",
    "#X_15 has about 1200 games\n",
    "\n",
    "#set the data frame and target\n",
    "X = X_15.copy()\n",
    "y = y_15.loc[:, 'won'].copy()\n",
    "\n",
    "model = gnb\n",
    "#model = sgdc\n",
    "##quick checks \n",
    "\n",
    "#for model in [gnb, sgdc]:  #partial_fit\n",
    "counter = 0\n",
    "acc_sum_train = 0\n",
    "acc_sum_test = 0\n",
    "for T in range(d, 1160 ,d):\n",
    "    y_train= y.iloc[T-d:T].copy()\n",
    "    y_test = y.iloc[T:T+d].copy()\n",
    "    #y_test = z.iloc[:100].copy()\n",
    "\n",
    "    X_train = X.iloc[T-d:T, :].copy()\n",
    "    X_test= X.iloc[T:T+d, :].copy()   \n",
    "    #X_test = W.iloc[:100].copy()\n",
    "    \n",
    "    #do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "    X_train.loc[:, columns_to_scale] = scal.fit_transform(X_train.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    #fit the scaler from train portion to the test portion \n",
    "    X_test.loc[:, columns_to_scale] = scal.transform(X_test.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    model.partial_fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(X_train) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    #print(' training error ', model, acc_train_err, ) #f1_train_err)\n",
    "   \n",
    "    \n",
    "    print(model, 'with partial_fit ', ' train error ', acc_train_err, ' f1_train ', f1_train_err, ' TEST ERROR ', acc, ' f1 ', f1)\n",
    "    acc_sum_test += acc\n",
    "    acc_sum_train += acc_train_err\n",
    "    counter +=1\n",
    "avg_acc = acc_sum_test/counter\n",
    "avg_acc_train = acc_sum_train/counter\n",
    "print(model, 'with partial_fit ', ' avg train error ', avg_acc_train, ' AVERAGE TEST ERROR ', avg_acc,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "47d68b54-673f-4fb0-aaa8-7770af1d815b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_lgr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-e870a52173ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for model in [gnb, sgdc]:  #partial_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m660\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpipe_lgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_win\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_win\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#y_pred1 = pipe_lgr.predict(X.iloc[d:, :])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe_lgr' is not defined"
     ]
    }
   ],
   "source": [
    "model = gnb\n",
    "##quick checks \n",
    "\n",
    "#for model in [gnb, sgdc]:  #partial_fit\n",
    "for d in range(20,660,20):\n",
    "    model.fit(X.iloc[:d :], y_win[:d])\n",
    "    model.fit(X.iloc[:d :], y_win[:d])\n",
    "    #y_pred1 = pipe_lgr.predict(X.iloc[d:, :])\n",
    "    y_pred = model.predict(X.iloc[d:, :])\n",
    "    y_test = y_win[d:].copy()\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(d, model, acc, f1)\n",
    "    \n",
    "    #print(d, pipe_lgr, acc1, f11)\n",
    "    #print(d, lgr, acc2, f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fc121-13de-48db-a73f-047ba759f3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ff98c50a-3a43-4b82-84b7-b43e5a0f8ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier()  train error  0.5933333333333334  f1_train  0.6369047619047619  TEST ERROR  0.55  f1  0.5714285714285715\n",
      "SGDClassifier()  train error  0.5706521739130435  f1_train  0.5820105820105821  TEST ERROR  0.55  f1  0.5714285714285714\n",
      "SGDClassifier()  train error  0.5872340425531914  f1_train  0.5782608695652174  TEST ERROR  0.7  f1  0.5\n",
      "SGDClassifier()  train error  0.5541666666666667  f1_train  0.6178571428571429  TEST ERROR  0.75  f1  0.8275862068965517\n",
      "SGDClassifier()  train error  0.5816326530612245  f1_train  0.6507666098807495  TEST ERROR  0.55  f1  0.64\n",
      "SGDClassifier()  train error  0.553  f1_train  0.5596059113300492  TEST ERROR  0.4  f1  0.33333333333333326\n",
      "SGDClassifier()  train error  0.5774509803921568  f1_train  0.6630179827990617  TEST ERROR  0.55  f1  0.7096774193548386\n",
      "SGDClassifier()  train error  0.5307692307692308  f1_train  0.5050709939148073  TEST ERROR  0.65  f1  0.5882352941176471\n",
      "SGDClassifier()  train error  0.5735849056603773  f1_train  0.6116838487972509  TEST ERROR  0.45  f1  0.4761904761904762\n",
      "SGDClassifier()  train error  0.6203703703703703  f1_train  0.6704180064308682  TEST ERROR  0.65  f1  0.6956521739130436\n"
     ]
    }
   ],
   "source": [
    "#T = 800  #train until this game in season (out of 1200)\n",
    "d = 20  #predict this many games \n",
    "scal = std_scal\n",
    "#X_15 has about 1200 games\n",
    "\n",
    "#set the data frame and target\n",
    "X = X_15.copy()\n",
    "y = y_15.loc[:, 'won'].copy()\n",
    "\n",
    "model = sgdc\n",
    "##quick checks \n",
    "\n",
    "#for model in [gnb, sgdc]:  #partial_fit\n",
    "for T in range(900, 1100,20):\n",
    "    y_train= y.iloc[:T].copy()\n",
    "    y_test = y.iloc[T:T+d].copy()\n",
    "    #y_test = z.iloc[:100].copy()\n",
    "\n",
    "    X_train = X.iloc[:T, :].copy()\n",
    "    X_test= X.iloc[T:T+d, :].copy()   \n",
    "    #X_test = W.iloc[:100].copy()\n",
    "    \n",
    "    #do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "    X_train.loc[:, columns_to_scale] = scal.fit_transform(X_train.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    #fit the scaler from train portion to the test portion \n",
    "    X_test.loc[:, columns_to_scale] = scal.transform(X_test.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(X_train) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    #print(' training error ', model, acc_train_err, ) #f1_train_err)\n",
    "   \n",
    "    \n",
    "    print(model, ' train error ', acc_train_err, ' f1_train ', f1_train_err, ' TEST ERROR ', acc, ' f1 ', f1)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e23b2f1b-53fd-4909-8948-375566c70e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB() with partial_fit   train error  0.5  f1_train  0.5  TEST ERROR  0.65  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.8  f1_train  0.8181818181818182  TEST ERROR  0.6  f1  0.6363636363636365\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6666666666666666  TEST ERROR  0.6  f1  0.6363636363636364\n",
      "GaussianNB() with partial_fit   train error  0.8  f1_train  0.8181818181818181  TEST ERROR  0.55  f1  0.5714285714285713\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.6666666666666665  TEST ERROR  0.55  f1  0.5714285714285714\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.7199999999999999  TEST ERROR  0.5  f1  0.4444444444444444\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.5263157894736842  TEST ERROR  0.5  f1  0.5454545454545454\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6666666666666666  TEST ERROR  0.6  f1  0.5555555555555556\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.7  TEST ERROR  0.75  f1  0.7368421052631577\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.7368421052631577  TEST ERROR  0.55  f1  0.5714285714285714\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.5714285714285714  TEST ERROR  0.65  f1  0.7199999999999999\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.7199999999999999  TEST ERROR  0.4  f1  0.4\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6363636363636365  TEST ERROR  0.6  f1  0.6363636363636364\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6956521739130435  TEST ERROR  0.7  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.7  f1_train  0.6666666666666666  TEST ERROR  0.35  f1  0.380952380952381\n",
      "GaussianNB() with partial_fit   train error  0.4  f1_train  0.4000000000000001  TEST ERROR  0.55  f1  0.6086956521739131\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.8  TEST ERROR  0.55  f1  0.5714285714285713\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6666666666666666  TEST ERROR  0.5  f1  0.5454545454545455\n",
      "GaussianNB() with partial_fit   train error  0.6  f1_train  0.6  TEST ERROR  0.65  f1  0.7407407407407406\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.7407407407407406  TEST ERROR  0.7  f1  0.6666666666666666\n",
      "GaussianNB() with partial_fit   train error  0.75  f1_train  0.7058823529411765  TEST ERROR  0.65  f1  0.631578947368421\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.631578947368421  TEST ERROR  0.55  f1  0.5714285714285715\n",
      "GaussianNB() with partial_fit   train error  0.65  f1_train  0.6956521739130435  TEST ERROR  0.55  f1  0.608695652173913\n",
      "GaussianNB() with partial_fit   train error  0.55  f1_train  0.608695652173913  TEST ERROR  0.45  f1  0.4761904761904762\n"
     ]
    }
   ],
   "source": [
    "#T = 800  #train until this game in season (out of 1200)\n",
    "d = 20  #predict this many games \n",
    "scal = std_scal\n",
    "#X_15 has about 1200 games\n",
    "\n",
    "#set the data frame and target\n",
    "X = X_15.copy()\n",
    "y = y_15.loc[:, 'won'].copy()\n",
    "\n",
    "model = gnb\n",
    "##quick checks \n",
    "\n",
    "#for model in [gnb, sgdc]:  #partial_fit\n",
    "for T in range(d, 500 ,d):\n",
    "    y_train= y.iloc[T-d:T].copy()\n",
    "    y_test = y.iloc[T:T+d].copy()\n",
    "    #y_test = z.iloc[:100].copy()\n",
    "\n",
    "    X_train = X.iloc[T-d:T, :].copy()\n",
    "    X_test= X.iloc[T:T+d, :].copy()   \n",
    "    #X_test = W.iloc[:100].copy()\n",
    "    \n",
    "    #do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "    X_train.loc[:, columns_to_scale] = scal.fit_transform(X_train.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    #fit the scaler from train portion to the test portion \n",
    "    X_test.loc[:, columns_to_scale] = scal.transform(X_test.loc[:, columns_to_scale]).copy()\n",
    "    \n",
    "    model.partial_fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(X_train) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    #print(' training error ', model, acc_train_err, ) #f1_train_err)\n",
    "   \n",
    "    \n",
    "    print(model, 'with partial_fit ', ' train error ', acc_train_err, ' f1_train ', f1_train_err, ' TEST ERROR ', acc, ' f1 ', f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a622276d-ca37-4e0f-91db-ec1802422b36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe_lgr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-e870a52173ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#for model in [gnb, sgdc]:  #partial_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m660\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpipe_lgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_win\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_win\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#y_pred1 = pipe_lgr.predict(X.iloc[d:, :])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe_lgr' is not defined"
     ]
    }
   ],
   "source": [
    "model = gnb\n",
    "##quick checks \n",
    "\n",
    "#for model in [gnb, sgdc]:  #partial_fit\n",
    "for d in range(20,660,20):\n",
    "    model.fit(X.iloc[:d :], y_win[:d])\n",
    "    model.fit(X.iloc[:d :], y_win[:d])\n",
    "    #y_pred1 = pipe_lgr.predict(X.iloc[d:, :])\n",
    "    y_pred = model.predict(X.iloc[d:, :])\n",
    "    y_test = y_win[d:].copy()\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(d, model, acc, f1)\n",
    "    \n",
    "    #print(d, pipe_lgr, acc1, f11)\n",
    "    #print(d, lgr, acc2, f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bd45e-bd4c-4f88-9632-a87cd5e6454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##example from stack overflow how to do multiple variable line graphs ... \n",
    "\n",
    "num_rows = 20\n",
    "years = list(range(1990, 1990 + num_rows))\n",
    "data_preproc = pd.DataFrame({\n",
    "    'Year': years, \n",
    "    'A': np.random.randn(num_rows).cumsum(),\n",
    "    'B': np.random.randn(num_rows).cumsum(),\n",
    "    'C': np.random.randn(num_rows).cumsum(),\n",
    "    'D': np.random.randn(num_rows).cumsum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe96720-1917-4326-9026-37776ebcf310",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fdf8fb-a91d-4473-b0ea-e428b2859688",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(data_preproc, ['Year'])[0:3]  ##seaborn  WHY? would you do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e32044-aa37-4fa3-80df-59d8a38b18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_theme(style='darkgrid', context='talk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75cadf-54b7-4931-8ad9-92ccfedd2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='Year', y='value', hue='variable', \n",
    "             data=pd.melt(data_preproc, ['Year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689b6c7-39d0-4c4c-a8f2-1640ae447186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_model_results(lgr, model_name ='logistic', X, dates, step, window_size, prediction_size, drop_first_k_days = 0): "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
