{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b210964-63eb-4766-bac4-d91ee0dfbfe1",
   "metadata": {},
   "source": [
    "##I am splitting v3_Clean_model_add_Pis_feat.ipynb into 2 notebooks \n",
    "\n",
    "-this one on modelling and \n",
    "\n",
    "-another one on creating stats data set (just Pisch for now)\n",
    "v1_stats_tools_Pish.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252a81d7-900b-46e1-9c38-4f1c9cdb3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "##part 1\n",
    "##use models with default\n",
    "##use data set with H/A +1, -1\n",
    "##do full window for now\n",
    "\n",
    "##next:\n",
    "##check if just excluding first 10 days helps (chaotic)\n",
    "##check if different windows help\n",
    "\n",
    "##next\n",
    "## can try tuning (for loops by hand, or ... use grid_search (use ML mastery code))\n",
    "##-I think tuning will be faster ... just do by hand ... loop over the possible things \n",
    "##-ONE for loop over i = (a,b,c,d)... for each model i[0]\n",
    "\n",
    "##Orrr can try adding features ... here we have to worry about:\n",
    "##-adding basic features eg pp, and correct fo%\n",
    "##-scaling numericals\n",
    "##-dummy vars for categoricals (are there any?) besides H/A\n",
    "##-num_windows and which lengths for moving avgs\n",
    "##-filtering the features for increasing complexity inteligently\n",
    "##-There is a dicotemy: \n",
    "##(a)use H/A + numerics or  ... here I think it can be made more like time-series\n",
    "##(b) just use mumerics (moving avg) ... here I think the order of the games is not important (note Leung did this, and random train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778fca46-90be-4c9f-a057-1e41ffea0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "##couple evaluation functions ##removed model_name as variable\n",
    "def evaluate_binary_classification(y_test, y_pred, y_proba=None, graph = False):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    #try:\n",
    "    if y_proba != None:\n",
    "        rocauc_score = roc_auc_score(y_test, y_proba)\n",
    "    else:\n",
    "        rocauc_score = \"no roc\"\n",
    "    #except: \n",
    "    #    pass     \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    if graph == True:\n",
    "        sns.heatmap(cm, annot=True)\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'{model_name}', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.show()\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1 score: \", f1)\n",
    "    print(\"rocauc: \", rocauc_score)\n",
    "    print(cm)\n",
    "    #return accuracy, precision, recall, f1, rocauc_score\n",
    "\n",
    "def evaluate_regression(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"mae\", mae)\n",
    "    print(\"mse\", mse)\n",
    "    print('r2', r2)\n",
    "    \n",
    "##display null values\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18af7514-a756-444a-bd2f-4de970ba321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_model_results(model, model_name, X, dates, step, window_size, prediction_size, drop_first_k_days = 0): #X = data \n",
    "    results_dic['model_name'] = []\n",
    "    results_dic['date'] = []\n",
    "    results_dic['mae'] = []\n",
    "    results_dic['mse'] = []\n",
    "    results_dic['r2'] = []   \n",
    "    \n",
    "    \n",
    "    #drop first k days from dates and X\n",
    "    dates = dates[drop_first_k_days :]\n",
    "    X = X.loc[X['full_date'].isin(dates), :].copy()  \n",
    "\n",
    "    for i in range(step, len(dates), step): ##eg step =10, so 17 rounds\n",
    "        \n",
    "        model.fit(X.loc[X['full_date'].isin(dates[max(i-window_size ,0):i]), :],y.loc[y['full_date'].isin(dates[max(i-window_size,0):i]),'goal_difference' ])\n",
    "        y_pred = model.predict(X.loc[X['full_date'].isin(dates[i:i+prediction_size]), :])\n",
    "        y_test = y.loc[y['full_date'].isin(dates[i:i+prediction_size]),'goal_difference' ]\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results_dic['model_name'].append(model_name)\n",
    "        results_dic['date'].append(dates[i])\n",
    "    \n",
    "        results_dic['mae'].append(mae)\n",
    "        results_dic['mse'].append(mse)\n",
    "        results_dic['r2'].append(r2)\n",
    "        \n",
    "    return results_dic #!\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7ec9a6-f01d-47c0-b2d1-02902750ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_model_results(model, model_name, X, dates, step, window_size, prediction_size, drop_first_k_days = 0): #X = data \n",
    "    results_dic ={}\n",
    "    results_dic['model_name'] = []\n",
    "    results_dic['date'] = []\n",
    "    results_dic['accuracy'] = []\n",
    "    results_dic['f1_score'] = []\n",
    "\n",
    "    #results_dic['precision'] = []\n",
    "  #  results_dic['recall'] = []\n",
    "    \n",
    "    #drop first k days from dates and X\n",
    "    dates = dates[drop_first_k_days :]\n",
    "    X = X.loc[X['full_date'].isin(dates), :].copy()  \n",
    "\n",
    "    for i in range(step, len(dates), step): ##eg step =10, so 17 rounds\n",
    "        model.fit(X.loc[X['full_date'].isin(dates[max(i-window_size ,0):i]), :],y.loc[y['full_date'].isin(dates[max(i-window_size,0):i]),'won' ])\n",
    "        y_pred = model.predict(X.loc[X['full_date'].isin(dates[i:i+prediction_size]), :])\n",
    "        y_test = y.loc[y['full_date'].isin(dates[i:i+prediction_size]),'won' ]\n",
    "    \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        #recision = precision_score(y_test, y_pred, zero_division = 0)\n",
    "        #recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred) #, average = None)\n",
    "            \n",
    "        results_dic['model_name'].append(model_name)  #append same model name every iter so same length as others\n",
    "        results_dic['date'].append(dates[i])\n",
    "                          \n",
    "        results_dic['accuracy'].append(accuracy)\n",
    "        results_dic['f1_score'].append(f1)\n",
    "        #results_dic['precision'].append(precision)\n",
    "        #results_dic['recall'].append(recall)\n",
    "    results_dic['model_name'].append('model_name'+'_avg')  #append same model name every iter so same length as others\n",
    "    results_dic['date'].append('average')\n",
    "    results_dic['accuracy'].append(round(np.mean(np.array(results_dic['accuracy'])), 2) ) \n",
    "    results_dic['f1_score'].append(round(np.mean(np.array(results_dic['f1_score'])), 2) ) \n",
    "    return results_dic #!\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c935eaa1-b629-406d-94f1-147c6daed626",
   "metadata": {},
   "outputs": [],
   "source": [
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cbaf7e-46af-4a63-9a9c-bbe35291577d",
   "metadata": {},
   "source": [
    "##TUNING INFO \n",
    "\n",
    "\n",
    "##hyper_parameters from here \n",
    "##https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "##for xgboost from here \n",
    "##https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/\n",
    "\n",
    "#xgb\n",
    "\n",
    "trees = [10, 50, 100, 500, 1000, 5000]  #100  #num of trees\n",
    "max_depth = range(1,11)  ##3-5\n",
    "rates = [0.0001, 0.001, 0.01, 0.1, 1.0]  #0.1\n",
    "subsample in arange(0.1, 1.1, 0.1):  #0.4, 0.5  ##this is 0.1, 0.2 ... 1.0 # % of features to sample\n",
    "\n",
    "\n",
    "#svc \n",
    "kernels in [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’] #if you use poly, then adjust degree\n",
    "C in [100, 10, 1.0, 0.1, 0.001]\n",
    "\n",
    "#gb\n",
    "\n",
    "learning_rate in [0.001, 0.01, 0.1]\n",
    "n_estimators [10, 100, 1000]\n",
    "subsample in [0.5, 0.7, 1.0]\n",
    "max_depth in [3, 7, 9]\n",
    "\n",
    "\n",
    "#rfc\n",
    "max_features [1 to 20]  #key\n",
    "max_features in [‘sqrt’, ‘log2’]\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "#bc\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "svm_dic = {'kernels':[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’]}\n",
    "lrc_dic = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "lgr_hp_dic = {'solver': [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’], 'penalty' : [‘none’, ‘l1’, ‘l2’, ‘elasticnet’],\n",
    "'C' :[100, 10, 1.0, 0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbb7207-4cc5-47b0-aa6b-7ba80d933695",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC(kernel = 'rbf')\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=5, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b36d368b-45fc-44c1-a5ff-579fea4fe286",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Note_books/Explore_Models/data_dummies_Pis_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38fe6f08-f93a-4e4d-a225-f4e4cf4160d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns ={'win%':'win%_cumul'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43d1417b-2139-46ca-a398-e65d0d403a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'won', 'goal_difference', 'Open', 'game_id', 'full_date',\n",
       "       'date', 'ANA', 'ARI', 'BOS', 'BUF', 'CAR', 'CBJ', 'CGY', 'CHI', 'COL',\n",
       "       'DAL', 'DET', 'EDM', 'FLA', 'LAK', 'MIN', 'MTL', 'NJD', 'NSH', 'NYI',\n",
       "       'NYR', 'OTT', 'PHI', 'PIT', 'SJS', 'STL', 'TBL', 'TOR', 'VAN', 'WPG',\n",
       "       'WSH', 'goalsAgainst_cumul_sum', 'goalsFor_cumul_sum',\n",
       "       'goalsDiff_cumul_sum', 'goalsFor%_cumul_avg', 'pp%_cumul_avg',\n",
       "       'pk%_cumul_avg', 'sh%_cumul_avg', 'sv%_cumul_avg', 'PDO_cumul_avg',\n",
       "       'fenwickPercentage_cumul_avg', 'last_10_games_win%', 'win%_cumul'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8cf071-2df8-41a3-8d37-cb37b9e3053e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ANA', 'ARI', 'BOS', 'BUF', 'CAR', 'CBJ', 'CGY', 'CHI', 'COL', 'DAL',\n",
       "       'DET', 'EDM', 'FLA', 'LAK', 'MIN', 'MTL', 'NJD', 'NSH', 'NYI', 'NYR',\n",
       "       'OTT', 'PHI', 'PIT', 'SJS', 'STL', 'TBL', 'TOR', 'VAN', 'WPG', 'WSH',\n",
       "       'goalsAgainst_cumul_sum', 'goalsFor_cumul_sum', 'goalsDiff_cumul_sum',\n",
       "       'goalsFor%_cumul_avg', 'pp%_cumul_avg', 'pk%_cumul_avg',\n",
       "       'sh%_cumul_avg', 'sv%_cumul_avg', 'PDO_cumul_avg',\n",
       "       'fenwickPercentage_cumul_avg', 'last_10_games_win%', 'win%_cumul'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.iloc[:10,7:].columns   ##columns are all safe ... \n",
    "#win%_cumul HAS to be previous day (NOT including day of ... o/w model can inspect \n",
    "#which teams win% went up and which ... actually kinda tough bec it's difference )\n",
    "##anyway I checked in v1_stats 'SJS' ... that win% = win%_cumul is *strictly* the previous days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d3ad79d-3202-44d4-8436-b6523d347b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>won</th>\n",
       "      <th>goal_difference</th>\n",
       "      <th>Open</th>\n",
       "      <th>game_id</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date</th>\n",
       "      <th>ANA</th>\n",
       "      <th>ARI</th>\n",
       "      <th>BOS</th>\n",
       "      <th>...</th>\n",
       "      <th>goalsDiff_cumul_sum</th>\n",
       "      <th>goalsFor%_cumul_avg</th>\n",
       "      <th>pp%_cumul_avg</th>\n",
       "      <th>pk%_cumul_avg</th>\n",
       "      <th>sh%_cumul_avg</th>\n",
       "      <th>sv%_cumul_avg</th>\n",
       "      <th>PDO_cumul_avg</th>\n",
       "      <th>fenwickPercentage_cumul_avg</th>\n",
       "      <th>last_10_games_win%</th>\n",
       "      <th>win%_cumul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-125</td>\n",
       "      <td>2012020011</td>\n",
       "      <td>20130119</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-130</td>\n",
       "      <td>2012020012</td>\n",
       "      <td>20130119</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-175</td>\n",
       "      <td>2012020013</td>\n",
       "      <td>20130119</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-115</td>\n",
       "      <td>2012020014</td>\n",
       "      <td>20130120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-115</td>\n",
       "      <td>2012020015</td>\n",
       "      <td>20130120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.063492</td>\n",
       "      <td>-0.051198</td>\n",
       "      <td>-0.114690</td>\n",
       "      <td>-0.07510</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-116</td>\n",
       "      <td>2012020016</td>\n",
       "      <td>20130120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-145</td>\n",
       "      <td>2012020017</td>\n",
       "      <td>20130120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.039560</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>-0.038635</td>\n",
       "      <td>0.06210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-150</td>\n",
       "      <td>2012020018</td>\n",
       "      <td>20130120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-105</td>\n",
       "      <td>2012020019</td>\n",
       "      <td>20130120</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.152273</td>\n",
       "      <td>-0.058608</td>\n",
       "      <td>-0.210881</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-197</td>\n",
       "      <td>2012020020</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.052521</td>\n",
       "      <td>0.060489</td>\n",
       "      <td>0.113010</td>\n",
       "      <td>0.11790</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-105</td>\n",
       "      <td>2012020021</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.178571</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.123839</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>-0.095268</td>\n",
       "      <td>-0.01710</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-109</td>\n",
       "      <td>2012020022</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.107843</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.179272</td>\n",
       "      <td>-0.11700</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-115</td>\n",
       "      <td>2012020023</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.045028</td>\n",
       "      <td>0.023511</td>\n",
       "      <td>-0.021517</td>\n",
       "      <td>-0.08070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-140</td>\n",
       "      <td>2012020024</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.091892</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.103797</td>\n",
       "      <td>0.23470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2012020025</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.179272</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-116</td>\n",
       "      <td>2012020026</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.237981</td>\n",
       "      <td>-0.034483</td>\n",
       "      <td>-0.272464</td>\n",
       "      <td>0.13440</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-170</td>\n",
       "      <td>2012020027</td>\n",
       "      <td>20130122</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.178571</td>\n",
       "      <td>0.063624</td>\n",
       "      <td>-0.103898</td>\n",
       "      <td>-0.040274</td>\n",
       "      <td>0.02440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-135</td>\n",
       "      <td>2012020028</td>\n",
       "      <td>20130122</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.380952</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>-0.303571</td>\n",
       "      <td>-0.122118</td>\n",
       "      <td>-0.104545</td>\n",
       "      <td>-0.226664</td>\n",
       "      <td>0.25525</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-148</td>\n",
       "      <td>2012020029</td>\n",
       "      <td>20130122</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>-0.275000</td>\n",
       "      <td>-0.054545</td>\n",
       "      <td>-0.010964</td>\n",
       "      <td>-0.065510</td>\n",
       "      <td>0.12805</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-120</td>\n",
       "      <td>2012020030</td>\n",
       "      <td>20130122</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.082327</td>\n",
       "      <td>0.05670</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  won  goal_difference  Open     game_id  full_date  date  ANA  \\\n",
       "10          10    1              1.0  -125  2012020011   20130119   119    0   \n",
       "11          11    1              2.0  -130  2012020012   20130119   119    0   \n",
       "12          12    0             -4.0  -175  2012020013   20130119   119    1   \n",
       "13          13    1              3.0  -115  2012020014   20130120   120    0   \n",
       "14          14    0             -3.0  -115  2012020015   20130120   120    0   \n",
       "15          15    0             -3.0  -116  2012020016   20130120   120    0   \n",
       "16          16    1              1.0  -145  2012020017   20130120   120    0   \n",
       "17          17    0              0.0  -150  2012020018   20130120   120    0   \n",
       "18          18    0             -2.0  -105  2012020019   20130120   120    0   \n",
       "19          19    1              0.0  -197  2012020020   20130121   121    0   \n",
       "20          20    1              1.0  -105  2012020021   20130121   121    0   \n",
       "21          21    0              0.0  -109  2012020022   20130121   121    0   \n",
       "22          22    0             -1.0  -115  2012020023   20130121   121    0   \n",
       "23          23    1              4.0  -140  2012020024   20130121   121    0   \n",
       "24          24    0              0.0   105  2012020025   20130121   121    0   \n",
       "25          25    0             -1.0  -116  2012020026   20130121   121    1   \n",
       "26          26    0             -2.0  -170  2012020027   20130122   122    0   \n",
       "27          27    0             -3.0  -135  2012020028   20130122   122    0   \n",
       "28          28    1              3.0  -148  2012020029   20130122   122    0   \n",
       "29          29    1              3.0  -120  2012020030   20130122   122    0   \n",
       "\n",
       "    ARI  BOS  ...  goalsDiff_cumul_sum  goalsFor%_cumul_avg  pp%_cumul_avg  \\\n",
       "10    1    0  ...                  NaN                  NaN            NaN   \n",
       "11    0    0  ...                  NaN                  NaN            NaN   \n",
       "12    0    0  ...                  NaN                  NaN            NaN   \n",
       "13    0    0  ...                  NaN                  NaN            NaN   \n",
       "14    0    0  ...                 -4.0            -0.500000      -0.666667   \n",
       "15    0    0  ...                  NaN                  NaN            NaN   \n",
       "16    0    0  ...                  1.0             0.095238      -0.214286   \n",
       "17    0    0  ...                  NaN                  NaN            NaN   \n",
       "18   -1    0  ...                 -4.0            -0.285714       0.200000   \n",
       "19    0   -1  ...                  5.0             0.550000      -0.166667   \n",
       "20    0    0  ...                 -4.0            -0.333333      -0.178571   \n",
       "21    0    0  ...                 -6.0            -0.500000       0.200000   \n",
       "22    0    0  ...                 -2.0            -0.047619      -0.100000   \n",
       "23    0    0  ...                 -1.0            -0.033333       0.000000   \n",
       "24    0    0  ...                  6.0             0.500000       0.500000   \n",
       "25    0    0  ...                 -7.0            -0.500000      -0.666667   \n",
       "26    0    0  ...                  0.0            -0.016667       0.166667   \n",
       "27    0    0  ...                 -6.0            -0.380952      -0.214286   \n",
       "28    0    0  ...                 -1.0            -0.083333      -0.050000   \n",
       "29    0    0  ...                  6.0             0.398810      -0.125000   \n",
       "\n",
       "    pk%_cumul_avg  sh%_cumul_avg  sv%_cumul_avg  PDO_cumul_avg  \\\n",
       "10            NaN            NaN            NaN            NaN   \n",
       "11            NaN            NaN            NaN            NaN   \n",
       "12            NaN            NaN            NaN            NaN   \n",
       "13            NaN            NaN            NaN            NaN   \n",
       "14       0.000000      -0.063492      -0.051198      -0.114690   \n",
       "15            NaN            NaN            NaN            NaN   \n",
       "16       0.400000      -0.039560       0.000926      -0.038635   \n",
       "17            NaN            NaN            NaN            NaN   \n",
       "18      -0.500000      -0.152273      -0.058608      -0.210881   \n",
       "19       0.500000       0.052521       0.060489       0.113010   \n",
       "20       0.250000      -0.123839       0.028571      -0.095268   \n",
       "21      -0.500000      -0.107843      -0.071429      -0.179272   \n",
       "22       0.050000      -0.045028       0.023511      -0.021517   \n",
       "23      -0.166667      -0.091892      -0.011905      -0.103797   \n",
       "24      -0.200000       0.071429       0.107843       0.179272   \n",
       "25       0.166667      -0.237981      -0.034483      -0.272464   \n",
       "26      -0.178571       0.063624      -0.103898      -0.040274   \n",
       "27      -0.303571      -0.122118      -0.104545      -0.226664   \n",
       "28      -0.275000      -0.054545      -0.010964      -0.065510   \n",
       "29       0.333333       0.018427       0.063900       0.082327   \n",
       "\n",
       "    fenwickPercentage_cumul_avg  last_10_games_win%  win%_cumul  \n",
       "10                          NaN                 NaN         NaN  \n",
       "11                          NaN                 NaN         NaN  \n",
       "12                          NaN                 NaN         NaN  \n",
       "13                          NaN                 NaN         NaN  \n",
       "14                     -0.07510                -1.0        -1.0  \n",
       "15                          NaN                 NaN         NaN  \n",
       "16                      0.06210                 0.0         0.0  \n",
       "17                          NaN                 NaN         NaN  \n",
       "18                      0.00510                -1.0        -1.0  \n",
       "19                      0.11790                 1.0         1.0  \n",
       "20                     -0.01710                -1.0        -1.0  \n",
       "21                     -0.11700                -1.0        -1.0  \n",
       "22                     -0.08070                 0.0         0.0  \n",
       "23                      0.23470                 0.0         0.0  \n",
       "24                      0.11700                 1.0         1.0  \n",
       "25                      0.13440                -1.0        -1.0  \n",
       "26                      0.02440                 0.0         0.0  \n",
       "27                      0.25525                -0.5        -0.5  \n",
       "28                      0.12805                -0.5        -0.5  \n",
       "29                      0.05670                 1.0         1.0  \n",
       "\n",
       "[20 rows x 49 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[10:30, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "725f861a-a646-432f-bb32-28ec219123b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20130119,\n",
       " 20130120,\n",
       " 20130121,\n",
       " 20130122,\n",
       " 20130123,\n",
       " 20130124,\n",
       " 20130125,\n",
       " 20130126,\n",
       " 20130127,\n",
       " 20130128]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exclude 1st couple dates of season to get rid of nan values ... might want to do more too... \n",
    "\n",
    "sorted(set(data['full_date']))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a781f6b4-4942-4074-b0ec-c76d6e867400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(data['full_date'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "72c4f528-8e7a-4ac4-8a94-fcab4e24e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "82271229-5da3-4c35-83c5-4f95b14aa8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9164d987-a91d-4a83-b4f7-5ce951de9545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.72964685, -1.14678356, -0.99992151, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [-1.72483559, -1.14678356, -1.43493269, ...,         nan,\n",
       "                nan,         nan],\n",
       "       [-1.72002434, -1.14678356, -1.43493269, ...,         nan,\n",
       "                nan,         nan],\n",
       "       ...,\n",
       "       [ 1.72002434, -1.14678356, -0.99992151, ..., -0.3782786 ,\n",
       "         0.35753988,  0.87202756],\n",
       "       [ 1.72483559,  0.87200413,  0.305112  , ...,  0.51466204,\n",
       "         0.01820178,  0.0906091 ],\n",
       "       [ 1.72964685, -1.14678356, -0.99992151, ...,  0.24483319,\n",
       "        -0.32113632,  0.35108192]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7783bf72-2c13-4c90-b664-f0ae4a1dd66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>won</th>\n",
       "      <th>goal_difference</th>\n",
       "      <th>Open</th>\n",
       "      <th>game_id</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date</th>\n",
       "      <th>ANA</th>\n",
       "      <th>ARI</th>\n",
       "      <th>BOS</th>\n",
       "      <th>...</th>\n",
       "      <th>goalsDiff_cumul_sum</th>\n",
       "      <th>goalsFor%_cumul_avg</th>\n",
       "      <th>pp%_cumul_avg</th>\n",
       "      <th>pk%_cumul_avg</th>\n",
       "      <th>sh%_cumul_avg</th>\n",
       "      <th>sv%_cumul_avg</th>\n",
       "      <th>PDO_cumul_avg</th>\n",
       "      <th>fenwickPercentage_cumul_avg</th>\n",
       "      <th>last_10_games_win%</th>\n",
       "      <th>win%_cumul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-197</td>\n",
       "      <td>2012020020</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.052521</td>\n",
       "      <td>0.060489</td>\n",
       "      <td>0.113010</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-105</td>\n",
       "      <td>2012020021</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.178571</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.123839</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>-0.095268</td>\n",
       "      <td>-0.017100</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-109</td>\n",
       "      <td>2012020022</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.107843</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.179272</td>\n",
       "      <td>-0.117000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-115</td>\n",
       "      <td>2012020023</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.047619</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.045028</td>\n",
       "      <td>0.023511</td>\n",
       "      <td>-0.021517</td>\n",
       "      <td>-0.080700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-140</td>\n",
       "      <td>2012020024</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.091892</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>-0.103797</td>\n",
       "      <td>0.234700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>715</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2012020717</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-0.035630</td>\n",
       "      <td>-0.007143</td>\n",
       "      <td>-0.042334</td>\n",
       "      <td>0.015628</td>\n",
       "      <td>-0.011179</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>-0.064266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>716</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2012020718</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-0.115817</td>\n",
       "      <td>0.043558</td>\n",
       "      <td>-0.022948</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>-0.004858</td>\n",
       "      <td>-0.005587</td>\n",
       "      <td>-0.065536</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.170213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>717</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-155</td>\n",
       "      <td>2012020719</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.069791</td>\n",
       "      <td>0.065907</td>\n",
       "      <td>-0.002668</td>\n",
       "      <td>0.022931</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.025092</td>\n",
       "      <td>-0.018613</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>718</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-150</td>\n",
       "      <td>2012020720</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.029841</td>\n",
       "      <td>-0.001672</td>\n",
       "      <td>-0.038180</td>\n",
       "      <td>0.021633</td>\n",
       "      <td>-0.019687</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.023549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>719</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-230</td>\n",
       "      <td>2012020624</td>\n",
       "      <td>20130428</td>\n",
       "      <td>428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.076134</td>\n",
       "      <td>0.009929</td>\n",
       "      <td>-0.014843</td>\n",
       "      <td>0.011085</td>\n",
       "      <td>-0.007410</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.010809</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.085106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>701 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  won  goal_difference  Open     game_id  full_date  date  ANA  \\\n",
       "19           19    1              0.0  -197  2012020020   20130121   121    0   \n",
       "20           20    1              1.0  -105  2012020021   20130121   121    0   \n",
       "21           21    0              0.0  -109  2012020022   20130121   121    0   \n",
       "22           22    0             -1.0  -115  2012020023   20130121   121    0   \n",
       "23           23    1              4.0  -140  2012020024   20130121   121    0   \n",
       "..          ...  ...              ...   ...         ...        ...   ...  ...   \n",
       "715         715    0             -3.0   105  2012020717   20130427   427    0   \n",
       "716         716    1              5.0   105  2012020718   20130427   427    0   \n",
       "717         717    0             -2.0  -155  2012020719   20130427   427   -1   \n",
       "718         718    1              1.0  -150  2012020720   20130427   427    0   \n",
       "719         719    0             -2.0  -230  2012020624   20130428   428    0   \n",
       "\n",
       "     ARI  BOS  ...  goalsDiff_cumul_sum  goalsFor%_cumul_avg  pp%_cumul_avg  \\\n",
       "19     0   -1  ...                  5.0             0.550000      -0.166667   \n",
       "20     0    0  ...                 -4.0            -0.333333      -0.178571   \n",
       "21     0    0  ...                 -6.0            -0.500000       0.200000   \n",
       "22     0    0  ...                 -2.0            -0.047619      -0.100000   \n",
       "23     0    0  ...                 -1.0            -0.033333       0.000000   \n",
       "..   ...  ...  ...                  ...                  ...            ...   \n",
       "715    0    0  ...                -19.0            -0.035630      -0.007143   \n",
       "716    0    0  ...                -26.0            -0.115817       0.043558   \n",
       "717    1    0  ...                 27.0             0.069791       0.065907   \n",
       "718    0    0  ...                 12.0             0.029841      -0.001672   \n",
       "719    0   -1  ...                 13.0             0.076134       0.009929   \n",
       "\n",
       "     pk%_cumul_avg  sh%_cumul_avg  sv%_cumul_avg  PDO_cumul_avg  \\\n",
       "19        0.500000       0.052521       0.060489       0.113010   \n",
       "20        0.250000      -0.123839       0.028571      -0.095268   \n",
       "21       -0.500000      -0.107843      -0.071429      -0.179272   \n",
       "22        0.050000      -0.045028       0.023511      -0.021517   \n",
       "23       -0.166667      -0.091892      -0.011905      -0.103797   \n",
       "..             ...            ...            ...            ...   \n",
       "715      -0.042334       0.015628      -0.011179       0.004450   \n",
       "716      -0.022948      -0.000730      -0.004858      -0.005587   \n",
       "717      -0.002668       0.022931       0.002161       0.025092   \n",
       "718      -0.038180       0.021633      -0.019687       0.001946   \n",
       "719      -0.014843       0.011085      -0.007410       0.003675   \n",
       "\n",
       "     fenwickPercentage_cumul_avg  last_10_games_win%  win%_cumul  \n",
       "19                      0.117900                 1.0    1.000000  \n",
       "20                     -0.017100                -1.0   -1.000000  \n",
       "21                     -0.117000                -1.0   -1.000000  \n",
       "22                     -0.080700                 0.0    0.000000  \n",
       "23                      0.234700                 0.0    0.000000  \n",
       "..                           ...                 ...         ...  \n",
       "715                    -0.064266                 0.0   -0.021277  \n",
       "716                    -0.065536                -0.4   -0.170213  \n",
       "717                    -0.018613                 0.1    0.212766  \n",
       "718                     0.023549                 0.0    0.021277  \n",
       "719                     0.010809                -0.1    0.085106  \n",
       "\n",
       "[701 rows x 49 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.loc[(data['full_date'] >=  20130121), :].copy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17e6583c-abfd-454a-bd01-3d32e874c6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>won</th>\n",
       "      <th>goal_difference</th>\n",
       "      <th>Open</th>\n",
       "      <th>game_id</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-197</td>\n",
       "      <td>2012020020</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-105</td>\n",
       "      <td>2012020021</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-109</td>\n",
       "      <td>2012020022</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-115</td>\n",
       "      <td>2012020023</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-140</td>\n",
       "      <td>2012020024</td>\n",
       "      <td>20130121</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>715</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2012020717</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>716</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2012020718</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>717</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-155</td>\n",
       "      <td>2012020719</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>718</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-150</td>\n",
       "      <td>2012020720</td>\n",
       "      <td>20130427</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>719</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-230</td>\n",
       "      <td>2012020624</td>\n",
       "      <td>20130428</td>\n",
       "      <td>428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>701 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  won  goal_difference  Open     game_id  full_date  date\n",
       "19           19    1              0.0  -197  2012020020   20130121   121\n",
       "20           20    1              1.0  -105  2012020021   20130121   121\n",
       "21           21    0              0.0  -109  2012020022   20130121   121\n",
       "22           22    0             -1.0  -115  2012020023   20130121   121\n",
       "23           23    1              4.0  -140  2012020024   20130121   121\n",
       "..          ...  ...              ...   ...         ...        ...   ...\n",
       "715         715    0             -3.0   105  2012020717   20130427   427\n",
       "716         716    1              5.0   105  2012020718   20130427   427\n",
       "717         717    0             -2.0  -155  2012020719   20130427   427\n",
       "718         718    1              1.0  -150  2012020720   20130427   427\n",
       "719         719    0             -2.0  -230  2012020624   20130428   428\n",
       "\n",
       "[701 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X.iloc[:, :7].copy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "490b59a2-921c-4039-ace4-1a87a7ea41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[:, 7:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ace1b7bb-473b-48a6-a3d3-62f8096d2a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_regr = y['goal_difference'].copy()\n",
    "y_win = y['won'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0ddbe65a-0e05-4883-a3ac-4d863e9fe234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19     1\n",
       "20     1\n",
       "21     0\n",
       "22     0\n",
       "23     1\n",
       "      ..\n",
       "715    0\n",
       "716    1\n",
       "717    0\n",
       "718    1\n",
       "719    0\n",
       "Name: won, Length: 701, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_win\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f502f52c-56b4-44e3-9a00-8baf4e81559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4efe1314-e15e-415a-bb5a-d0bde3acf099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() 0.5561097256857855 0.6941580756013745\n",
      "RidgeClassifier() 0.5835411471321695 0.6693069306930693\n",
      "RandomForestClassifier(max_depth=5, random_state=0) 0.543640897755611 0.659217877094972\n",
      "GaussianNB() 0.5261845386533666 0.5869565217391304\n",
      "LogisticRegression(random_state=0) 0.5835411471321695 0.6782273603082851\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression', LogisticRegression())]) 0.5660847880299252 0.6506024096385543\n",
      "BaggingClassifier() 0.5087281795511222 0.5553047404063205\n",
      "GradientBoostingClassifier() 0.5561097256857855 0.6260504201680671\n",
      "[21:43:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None) 0.5211970074812967 0.5949367088607596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr,pipe_lgr , bc, gbc, xgbc]:\n",
    "    scaler.fit()\n",
    "    \n",
    "    model.fit(X.iloc[:300 :], y_win[:300])\n",
    "    y_pred = model.predict(X.iloc[300:, :])\n",
    "    y_test = y_win[300:].copy()\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    print(model, acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fee00-a8f0-44ce-a6d6-ca1d06531e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok pretty pretty good start ... logistic and ridgec 0.5835. (they are identical .. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df95e803-77c9-47f3-a160-4bb2b5787882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(701, 42)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4df6ad2b-c29e-4af4-a25c-6fdb32746a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe_lgr = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "47d68b54-673f-4fb0-aaa8-7770af1d815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 LogisticRegression(random_state=0) 0.4977973568281938 0.5327868852459017\n",
      "40 LogisticRegression(random_state=0) 0.5068078668683812 0.5788113695090439\n",
      "60 LogisticRegression(random_state=0) 0.5413416536661466 0.6334164588528678\n",
      "80 LogisticRegression(random_state=0) 0.5185185185185185 0.6121919584954604\n",
      "100 LogisticRegression(random_state=0) 0.5324459234608985 0.6355382619974059\n",
      "120 LogisticRegression(random_state=0) 0.5111876075731497 0.5988700564971752\n",
      "140 LogisticRegression(random_state=0) 0.5222816399286988 0.6081871345029239\n",
      "160 LogisticRegression(random_state=0) 0.5138632162661737 0.6021180030257187\n",
      "180 LogisticRegression(random_state=0) 0.5182341650671785 0.5958132045088567\n",
      "200 LogisticRegression(random_state=0) 0.5349301397205589 0.6084033613445378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 LogisticRegression(random_state=0) 0.5426195426195426 0.6126760563380281\n",
      "240 LogisticRegression(random_state=0) 0.559652928416486 0.6407079646017699\n",
      "260 LogisticRegression(random_state=0) 0.5827664399092971 0.6654545454545455\n",
      "280 LogisticRegression(random_state=0) 0.5724465558194775 0.6525096525096524\n",
      "300 LogisticRegression(random_state=0) 0.5835411471321695 0.6782273603082851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 LogisticRegression(random_state=0) 0.5748031496062992 0.6720647773279352\n",
      "340 LogisticRegression(random_state=0) 0.5817174515235457 0.6710239651416121\n",
      "360 LogisticRegression(random_state=0) 0.5777126099706745 0.6756756756756758\n",
      "380 LogisticRegression(random_state=0) 0.5763239875389408 0.6777251184834123\n",
      "400 LogisticRegression(random_state=0) 0.5548172757475083 0.654639175257732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 LogisticRegression(random_state=0) 0.5800711743772242 0.6758241758241759\n",
      "440 LogisticRegression(random_state=0) 0.5747126436781609 0.6725663716814159\n",
      "460 LogisticRegression(random_state=0) 0.5643153526970954 0.6645367412140575\n",
      "480 LogisticRegression(random_state=0) 0.5475113122171946 0.6503496503496504\n",
      "500 LogisticRegression(random_state=0) 0.5970149253731343 0.6896551724137931\n",
      "520 LogisticRegression(random_state=0) 0.585635359116022 0.6753246753246753\n",
      "540 LogisticRegression(random_state=0) 0.577639751552795 0.6530612244897959\n",
      "560 LogisticRegression(random_state=0) 0.5602836879432624 0.6352941176470589\n",
      "580 LogisticRegression(random_state=0) 0.5537190082644629 0.6399999999999999\n",
      "600 LogisticRegression(random_state=0) 0.594059405940594 0.6771653543307086\n",
      "620 LogisticRegression(random_state=0) 0.5185185185185185 0.6060606060606061\n",
      "640 LogisticRegression(random_state=0) 0.4918032786885246 0.5373134328358209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = lgr\n",
    "##quick checks \n",
    "for d in range(20,660,20):\n",
    "    pipe_lgr.fit(X.iloc[:d :], y_win[:d])\n",
    "    model.fit(X.iloc[:d :], y_win[:d])\n",
    "    #y_pred1 = pipe_lgr.predict(X.iloc[d:, :])\n",
    "    y_pred = model.predict(X.iloc[d:, :])\n",
    "    y_test = y_win[d:].copy()\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(d, model, acc, f1)\n",
    "    \n",
    "    #print(d, pipe_lgr, acc1, f11)\n",
    "    #print(d, lgr, acc2, f12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c4bd45e-bd4c-4f88-9632-a87cd5e6454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##example from stack overflow how to do multiple variable line graphs ... \n",
    "\n",
    "num_rows = 20\n",
    "years = list(range(1990, 1990 + num_rows))\n",
    "data_preproc = pd.DataFrame({\n",
    "    'Year': years, \n",
    "    'A': np.random.randn(num_rows).cumsum(),\n",
    "    'B': np.random.randn(num_rows).cumsum(),\n",
    "    'C': np.random.randn(num_rows).cumsum(),\n",
    "    'D': np.random.randn(num_rows).cumsum()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9fe96720-1917-4326-9026-37776ebcf310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>-0.438006</td>\n",
       "      <td>0.350888</td>\n",
       "      <td>1.799161</td>\n",
       "      <td>0.532655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>0.792898</td>\n",
       "      <td>-0.059380</td>\n",
       "      <td>2.505481</td>\n",
       "      <td>1.014502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992</td>\n",
       "      <td>1.944180</td>\n",
       "      <td>-0.645006</td>\n",
       "      <td>2.847458</td>\n",
       "      <td>1.806303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year         A         B         C         D\n",
       "0  1990 -0.438006  0.350888  1.799161  0.532655\n",
       "1  1991  0.792898 -0.059380  2.505481  1.014502\n",
       "2  1992  1.944180 -0.645006  2.847458  1.806303"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preproc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65fdf8fb-a91d-4473-b0ea-e428b2859688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990</td>\n",
       "      <td>A</td>\n",
       "      <td>-0.438006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991</td>\n",
       "      <td>A</td>\n",
       "      <td>0.792898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992</td>\n",
       "      <td>A</td>\n",
       "      <td>1.944180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year variable     value\n",
       "0  1990        A -0.438006\n",
       "1  1991        A  0.792898\n",
       "2  1992        A  1.944180"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.melt(data_preproc, ['Year'])[0:3]  ##seaborn  WHY? would you do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "66e32044-aa37-4fa3-80df-59d8a38b18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_theme(style='darkgrid', context='talk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ac75cadf-54b7-4931-8ad9-92ccfedd2891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Year', ylabel='value'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x='Year', y='value', hue='variable', \n",
    "             data=pd.melt(data_preproc, ['Year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689b6c7-39d0-4c4c-a8f2-1640ae447186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_model_results(lgr, model_name ='logistic', X, dates, step, window_size, prediction_size, drop_first_k_days = 0): "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
