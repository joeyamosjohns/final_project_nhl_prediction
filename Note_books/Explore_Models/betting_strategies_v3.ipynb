{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34da09a1-4fa6-41ec-8b5e-4feb04bb7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c75c22e-848f-4a4f-8d6c-d32797909c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5e3250-ba9b-4518-8a81-f30975061070",
   "metadata": {},
   "outputs": [],
   "source": [
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "std_scal = StandardScaler()\n",
    "mm_scal = MinMaxScaler()\n",
    "\n",
    "\n",
    "##no tuning \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cd78a3-04d6-435a-8952-356dac59f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['Unnamed: 0', 'game_id', 'mp_date', 'season', 'home_team', 'away_team',\n",
    "       'home_odds', 'away_odds', 'home_goals', 'away_goals', 'goal_diff_target', 'home_win',\n",
    "       'settled_in', ]\n",
    "\n",
    "x_cols = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']\n",
    "columns_to_scale = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']  ##same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0698730d-36ec-4473-a417-b9edf18de38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10385, 27)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Data/Shaped_Data/data_LJ.csv')\n",
    "\n",
    "X['season'] = X['season'].apply(int)\n",
    "X['game_id'] = X['game_id'].apply(int)\n",
    "X['mp_date'] = X['mp_date'].apply(int)\n",
    "X['goal_diff_target'] = X['home_goals'] - X['away_goals']\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96607d-1f35-49f7-b1fd-ece6605a0009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f88ea0a1-da26-4d58-86db-cfeca2059a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_early(X):\n",
    "    filt_no_early  = (X['mp_date'].apply(lambda x : x% 10**4) < 900) | (1100 < X['mp_date'].apply(lambda x : x% 10**4))\n",
    "    return X.loc[filt_no_early, : ].copy()  ## keep games < 900 and > 1100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7790a2-c07d-4413-b86c-bd525170749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = np.array(X.loc[(X['season'] <= 20152016), x_cols].copy())\n",
    "  #features test-train\n",
    "Y = X.loc[(X['season'] <= 20152016), y_cols].copy()\n",
    "   #targets\n",
    "y = np.array(Y['home_win']).reshape(-1,1)\n",
    "\n",
    "\n",
    "              \n",
    "x_16 = X.loc[(X['season'] == 20162017), x_cols].copy()  #features test-train\n",
    "Y_16 = X.loc[(X['season'] == 20162017), y_cols].copy()   #targets\n",
    "y_16 = np.array(Y_16['home_win']).reshape(-1,1)\n",
    "\n",
    "x_17 = X.loc[(X['season'] > 20162017), x_cols].copy()  #features test-train\n",
    "Y_17 = X.loc[(X['season'] > 20162017), y_cols].copy()   #targets\n",
    "y_17 = np.array(Y_17['home_win']).reshape(-1,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d81f6d-b7f2-41c6-935a-0903fb0994dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20162017    1060\n",
       "20172018    1052\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] >= 20162017), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1186beb4-91d6-47b9-8120-28a064d658f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20142015    1046\n",
       "20112012    1037\n",
       "20152016    1036\n",
       "20132014     994\n",
       "20082009     971\n",
       "20102011     962\n",
       "20092010     946\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95579dd2-f1f4-484a-8932-5e0c58f53b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6992"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85c08e0c-1a34-4216-b760-98e3602f6e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9104, 27)\n",
      "(6992, 14) (6992, 13) (6992, 1)\n",
      "(1060, 14) (1060, 1)\n",
      "(1052, 14) (1052, 1)\n",
      "sum  6992 1060 1052  is:  9104\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(x.shape, Y.shape, y.shape)\n",
    "print(x_16.shape, y_16.shape)\n",
    "print(x_17.shape, y_17.shape)\n",
    "print('sum ', y.shape[0], y_16.shape[0], y_17.shape[0], ' is: ', y.shape[0] + y_16.shape[0]+y_17.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8c0e8c-6fd4-4fa7-adb2-09de0a57be6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20142015    1046\n",
       "20112012    1037\n",
       "20152016    1036\n",
       "20132014     994\n",
       "20082009     971\n",
       "20102011     962\n",
       "20092010     946\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d646e4-ca06-4cf3-ae8b-0fec4683866d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20162017    1060\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_16['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93df204b-03a2-4e1d-bf85-452971611b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    1052\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_17['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eecf9232-215c-4c28-af05-e8f3afa5abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() no tuning TEST:  0.5589706933523946 0.6527855936972425 training :  0.5991417843733238 0.6873082287308229\n",
      "RidgeClassif no tuning TEST:  0.557541100786276 0.6492917847025497 training :  0.5817986769175756 0.672821373618688\n",
      "RandomForest no tuning TEST:  0.5589706933523946 0.6512153759185981 training :  0.5889504738065439 0.6790450928381963\n",
      "GaussianNB() no tuning TEST:  0.5432451751250893 0.5725752508361204 training :  0.5716073663507957 0.6109126339720689\n",
      "LogisticRegr no tuning TEST:  0.557541100786276 0.6480955088118249 training :  0.5794743429286608 0.6705882352941176\n",
      "BaggingClass no tuning TEST:  0.5225160829163689 0.541838134430727 training :  0.9849812265331664 0.9862249918005904\n",
      "GradientBoos no tuning TEST:  0.5568263045032166 0.6412037037037037 training :  0.6618988020740211 0.726457399103139\n",
      "[11:40:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifie no tuning TEST:  0.5325232308791994 0.6012195121951219 training :  0.9608439120328983 0.9649094696362762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "std_scal = StandardScaler()\n",
    "mm_scal = MinMaxScaler()\n",
    "\n",
    "\n",
    "##no tuning \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0, max_iter = 1000)\n",
    "svc = SVC(probability = True)\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "##quick checks \n",
    "#for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "def fit_model(model, print_results = True, tuned = False)    \n",
    "    model.fit(x_train_sc, y_train.ravel())\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    acct = accuracy_score(y_train, y_predt)\n",
    "    f1t = f1_score(y_train,y_predt)\n",
    "    if tuned:\n",
    "        tuned= 'WITH tuning'\n",
    "    else\n",
    "        tuned = 'NO tuning'\n",
    "    \n",
    "    if print_results:  \n",
    "        print(str(model)[0:12], tuned+ 'TEST acc, f1: ', acc, f1, 'training acct, f1t: ', acct, f1t)\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be5151e5-5b05-4af4-8ba4-9eb3e9d22edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.07000000e+00,  3.88836773e+00, -1.50203584e+00, ...,\n",
       "         0.00000000e+00,  6.54126895e+00, -2.47000000e+00],\n",
       "       [-5.33000000e+00,  3.22158462e-01,  7.96868447e-01, ...,\n",
       "        -8.69000000e+00,  7.75485350e-01,  2.28000000e+00],\n",
       "       [-1.13900000e+01,  1.29385506e+00, -7.31934901e-01, ...,\n",
       "        -1.05100000e+01,  2.16503340e+00, -2.47000000e+00],\n",
       "       ...,\n",
       "       [-1.07000000e+00, -1.70848125e-03, -6.54668512e-02, ...,\n",
       "        -2.04000000e+00, -3.28207435e-01,  5.00000000e-01],\n",
       "       [ 4.45000000e+00, -3.02955417e-01,  7.35640597e-01, ...,\n",
       "         4.42000000e+00, -2.41020059e-01,  1.24000000e+00],\n",
       "       [ 3.87000000e+00,  2.48248880e-01,  4.01685000e-01, ...,\n",
       "         6.00000000e+00,  3.36489035e-01,  3.90000000e-01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c760d5f2-245b-4abf-bc98-8e1510805cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() 20162017, no tuning, TEST:  0.5943396226415094 0.684287812041116\n",
      "RidgeClassif 20162017, no tuning, TEST:  0.5943396226415094 0.6838235294117646\n",
      "RandomForest 20162017, no tuning, TEST:  0.5962264150943396 0.6862170087976539\n",
      "GaussianNB() 20162017, no tuning, TEST:  0.5716981132075472 0.6092943201376936\n",
      "LogisticRegr 20162017, no tuning, TEST:  0.5952830188679246 0.6847905951506247\n",
      "BaggingClass 20162017, no tuning, TEST:  0.5452830188679245 0.5602189781021898\n",
      "GradientBoos 20162017, no tuning, TEST:  0.5792452830188679 0.6579754601226995\n",
      "XGBClassifie 20162017, no tuning, TEST:  0.5537735849056604 0.6119770303527481\n"
     ]
    }
   ],
   "source": [
    "#check on 2016\n",
    "\n",
    "x_test2 = std_scal.transform(x_16).copy()\n",
    "y_test2 = y_16.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], '20162017, no tuning, TEST: ', acc, f1) #, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc7e0de-5c4d-45c7-84fc-b9f7c774c747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() 20172018, no tuning, TEST:  0.5874524714828897 0.6917613636363635\n",
      "RidgeClassif 20172018, no tuning, TEST:  0.5855513307984791 0.6876790830945558\n",
      "RandomForest 20172018, no tuning, TEST:  0.6036121673003803 0.7090020935101187\n",
      "GaussianNB() 20172018, no tuning, TEST:  0.5731939163498099 0.628003314001657\n",
      "LogisticRegr 20172018, no tuning, TEST:  0.5884030418250951 0.691814946619217\n",
      "BaggingClass 20172018, no tuning, TEST:  0.5304182509505704 0.5711805555555556\n",
      "GradientBoos 20172018, no tuning, TEST:  0.594106463878327 0.6876371616678859\n",
      "XGBClassifie 20172018, no tuning, TEST:  0.5627376425855514 0.6423017107309487\n"
     ]
    }
   ],
   "source": [
    "#check on 2017\n",
    "\n",
    "x_test2 = std_scal.transform(x_17).copy()\n",
    "y_test2 = y_17.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], '20172018, no tuning, TEST: ', acc, f1) #, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11072809-01e1-4905-94d3-0e356355c0a0",
   "metadata": {},
   "source": [
    "#can do version where we get y regr version at beinning ... do all teh regression stuff ... \n",
    "then for later cells do y = v_win(y) for classifiers ... \n",
    "\n",
    "##regression models now ...\n",
    "\n",
    "y = Y['goal_diff_target'].copy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "lr = Ridge(alpha=50000) \n",
    "rfr = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "for model in [lr, rfr, xgbr]:\n",
    "    model.fit(x_train_sc, y_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    print(y_pred[0:5])\n",
    "    y_predw = v_win(y_pred)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    y_predwt = v_win(y_predt) \n",
    "    y_trainw = v_win(y_train)\n",
    "    y_testw = v_win(y_test)  #same as usual win/loss\n",
    "    acc = accuracy_score(y_testw, y_predw)\n",
    "    f1 = f1_score(y_testw,y_predw)\n",
    "    acct = accuracy_score(y_trainw, y_predwt)\n",
    "    f1t = f1_score(y_trainw,y_predwt)\n",
    "    \n",
    "  \n",
    "    print(str(model)[0:20], 'TEST: ', acc, f1 ,'training : ', acct, f1t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31a5eba7-807e-4e15-8aad-2833565bca41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, pr with tuning TEST:  0.5668334524660472 0.6666666666666667 training :  0.6264974074736277 0.7127732710023373\n",
      "RidgeClassif with tuning TEST:  0.5675482487491065 0.6606842400448683 training :  0.5758984444841766 0.6692693809258226\n",
      "RandomForest with tuning TEST:  0.5668334524660472 0.6521239954075775 training :  0.8119077418201323 0.8471818710052295\n",
      "GaussianNB() with tuning TEST:  0.5689778413152251 0.6045901639344262 training :  0.5649919542284999 0.606119475473531\n",
      "LogisticRegr with tuning TEST:  0.5661186561829878 0.6564799094510471 training :  0.573574110495262 0.6666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClass with tuning TEST:  0.5318084345961401 0.5888261142498431 training :  0.9998212050777758 0.9998372130880677\n",
      "GradientBoos with tuning TEST:  0.5725518227305219 0.6617647058823529 training :  0.6676202395851958 0.7381321312860967\n",
      "[11:40:34] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifie with tuning TEST:  0.5539671193709793 0.6450511945392492 training :  0.7201859467191132 0.7769066286528866\n"
     ]
    }
   ],
   "source": [
    "## some tries at tuning here \n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=12, stratify=y)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n",
    "\n",
    "\n",
    "##classifier models\n",
    "lrc2 = RidgeClassifier(alpha =0.2)\n",
    "gnb2 = GaussianNB()\n",
    "lgr2 = LogisticRegression(random_state = 0, C =10**5, max_iter = 500)\n",
    "svc2 = SVC(kernel = 'rbf', C =10, probability = True)  #why would you ever have it false? \n",
    "\n",
    "#tree-based classifiers\n",
    "rfc2 =  RandomForestClassifier(max_depth=10, random_state=0, n_estimators = 40)\n",
    "bc2 = BaggingClassifier(n_estimators  = 60, max_samples = 0.85)\n",
    "gbc2 = GradientBoostingClassifier(learning_rate =0.1, n_estimators = 40, max_depth =4 )\n",
    "xgbc2 = XGBClassifier(n_estimators= 45, eta=0.05)\n",
    "\n",
    "##quick checks \n",
    "for model in [svc2, lrc2, rfc2, gnb2, lgr2, bc2, gbc2, xgbc2]:\n",
    "    model.fit(x_train_sc, y_train.ravel())\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test_sc)\n",
    "    y_predt= model.predict(x_train_sc)  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    acct = accuracy_score(y_train, y_predt)\n",
    "    f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], 'with tuning TEST: ', acc, f1, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaeaeb99-763f-4885-b1a9-e6e83166f5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, pr 20162017, tuned, TEST:  0.5764150943396227 0.6715435259692759\n",
      "RidgeClassif 20162017, tuned, TEST:  0.5924528301886792 0.6855895196506551\n",
      "RandomForest 20162017, tuned, TEST:  0.5754716981132075 0.6631736526946107\n",
      "GaussianNB() 20162017, tuned, TEST:  0.5707547169811321 0.6094420600858369\n",
      "LogisticRegr 20162017, tuned, TEST:  0.5877358490566038 0.6807888970051132\n",
      "BaggingClass 20162017, tuned, TEST:  0.5490566037735849 0.6062602965403624\n",
      "GradientBoos 20162017, tuned, TEST:  0.5650943396226416 0.6567386448250185\n",
      "XGBClassifie 20162017, tuned, TEST:  0.5660377358490566 0.6541353383458647\n"
     ]
    }
   ],
   "source": [
    "#check on 2016\n",
    "\n",
    "x_test2 = std_scal.transform(x_16).copy()\n",
    "y_test2 = y_16.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "#for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "for model in [svc2, lrc2, rfc2, gnb2, lgr2, bc2, gbc2, xgbc2]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12], '20162017, tuned, TEST: ', acc, f1) #, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7360dc39-f8c2-44bb-98db-cfd561bd6879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, pr 20172018, tuned, TEST:  0.5836501901140685 0.6857962697274033\n",
      "RidgeClassif 20172018, tuned, TEST:  0.5960076045627376 0.6975088967971531\n",
      "RandomForest 20172018, tuned, TEST:  0.5931558935361216 0.6857562408223201\n",
      "GaussianNB() 20172018, tuned, TEST:  0.5674904942965779 0.6211490424646129\n",
      "LogisticRegr 20172018, tuned, TEST:  0.5960076045627376 0.6992215145081387\n",
      "BaggingClass 20172018, tuned, TEST:  0.5484790874524715 0.6141348497156783\n",
      "GradientBoos 20172018, tuned, TEST:  0.5950570342205324 0.6921965317919075\n",
      "XGBClassifie 20172018, tuned, TEST:  0.5988593155893536 0.6901615271659325\n"
     ]
    }
   ],
   "source": [
    "#check on 2017\n",
    "\n",
    "x_test2 = std_scal.transform(x_17).copy()\n",
    "y_test2 = y_17.copy()\n",
    "\n",
    "\n",
    "##quick checks \n",
    "#for model in [svc, lrc, rfc, gnb, lgr, bc, gbc, xgbc]:\n",
    "for model in [svc2, lrc2, rfc2, gnb2, lgr2, bc2, gbc2, xgbc2]:\n",
    "    #model.fit(x_train)\n",
    "      #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred= model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "    acc = accuracy_score(y_test2, y_pred)\n",
    "    f1 = f1_score(y_test2,y_pred)\n",
    "   \n",
    "    #acct = accuracy_score(y_train, y_predt)\n",
    "    #f1t = f1_score(y_train,y_predt)\n",
    "  \n",
    "  \n",
    "    print(str(model)[0:12],  '20172018, tuned, TEST: ', acc, f1) #, 'training : ', acct, f1t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbc0f2-6dff-4b02-9300-81ff3602e363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3ef2b-d5e5-4c64-9a59-37bec36ff938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb5e91-e1f2-4246-be3c-094c397a4f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28966856-2579-469a-aeaa-72f59b5be611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc38ca-eec8-40d8-a258-e2959bcaad15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d66b9ba-63ed-4767-9a36-01ec0e309e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go with lgr, lgr2 (tuned or not) for betting investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47fbd718-57c1-4429-95fe-108dc66a11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_16 = std_scal.transform(x_16).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac5684d-7b01-42c6-b882-27e83e74e40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52407527, 0.47592473],\n",
       "       [0.58086487, 0.41913513],\n",
       "       [0.41006043, 0.58993957],\n",
       "       ...,\n",
       "       [0.37652706, 0.62347294],\n",
       "       [0.44296909, 0.55703091],\n",
       "       [0.3999811 , 0.6000189 ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc2.predict_proba(x_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "901e2652-3f6a-4cab-aec9-3dc441a37d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00380656, 0.32217664, 0.21205196, ..., 0.07010458, 0.54824593,\n",
       "       0.05309209])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.predict_proba(x_16)[0:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4355c65a-6766-490a-bf4d-d4876b218902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55f4f1-e6c9-4913-840d-065449b6a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##make betting strategy on x_16 test it on x_17  ... and other seasons if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e61dc5-2f88-4b24-8fc2-3cee9bb55aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dic(model, year):\n",
    "    \n",
    "    \n",
    "    predict_proba = [round(x,4) for x in model.predict_proba(x_16)[0:, 0]]\n",
    "    \n",
    "    dic['lgr_pred'] = list(model.predict(x_16))\n",
    "    dic['actual'] = [ x[0] for x in list(y_16)]\n",
    "    dic['lgr_conf_0'] =[round(x,4) for x in model.predict_proba(x_16)[0:, 0]]\n",
    "    dic['lgr_conf_1'] = [round(x,4) for x in model.predict_proba(x_16)[0:, 1]]\n",
    "    dic['away_odds'] = list(Y_16['away_odds'])\n",
    "    dic['home_odds'] = list(Y_16['home_odds'])\n",
    "    df_svc = pd.DataFrame(dic_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f18d06-b7d7-4f7c-ac86-4fc13335362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_svc['away_impl_proba'] = v_impl_proba(df_svc['away_odds'])\n",
    "df_svc['home_impl_proba'] = v_impl_proba(df_svc['home_odds'])\n",
    "\n",
    "\n",
    "df_svc['home_model-odds'] = df_svc['lgr_conf_1'] - df_svc['home_impl_proba']\n",
    "df_svc['away_model-odds'] = df_svc['lgr_conf_0'] - df_svc['away_impl_proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aedaa2-3753-48be-9c6e-63591dafd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d00cc8-0c7b-4d38-892d-522af1ae339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implied_proba(odds):\n",
    "    if odds > 0: \n",
    "        return 100/(odds+100)    #bet 100 to get 100+odds; profit = odds\n",
    "    if odds < 0:\n",
    "        return (-odds)/(-odds + 100)   #bet |odds| to get 100+|odds|; profit = 100\n",
    "    \n",
    "v_impl_proba = np.vectorize(implied_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c162587f-686a-4a11-9957-de39ff56d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dic['lgr_pred'] = list(lgr.predict(x_16))\n",
    "dic['gnb_pred'] = list(gnb.predict(x_16))\n",
    "dic['svc_proba'] = list(svc.predict_proba(x_16)[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3906d9c-f5cc-48bf-ac0d-394863c8b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gnb.predict(x_17)\n",
    "actual = y_17\n",
    "confusionMatrix = confusion_matrix(actual, predictions)\n",
    "display(confusionMatrix)\n",
    "\n",
    "tn = confusionMatrix[0][0]\n",
    "fp = confusionMatrix[0][1]\n",
    "fn = confusionMatrix[1][0]\n",
    "tp = confusionMatrix[1][1]\n",
    "actualYes = fn + tp \n",
    "actualNo = tn + fp\n",
    "predictedYes = fp + tp\n",
    "predictedNo = tn + fn\n",
    "\n",
    "print('When home team wins, classifier predicts they will win %6.2f%% of the time' % (tp / actualYes * 100))\n",
    "print('When home team loses, classifier predicts they will win %6.2f%% of the time' % (fp / actualNo * 100))\n",
    "print('When home team loses, classifier predicts they will lose %6.2f%% of the time' % (tn / actualNo * 100))\n",
    "print('When classifer predicts home team will win, home team actually wins %6.2f%% of the time' % (tp / predictedYes * 100))\n",
    "print('When classifer predicts home team will lose, home team actually loses %6.2f%% of the time' % (tn / predictedNo * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2bf18-56b6-449d-90c4-1907eea18211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
