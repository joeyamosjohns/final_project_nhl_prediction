{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b93318-b204-4321-a902-e019ed05e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###let's try to predeict to two seasons ... \n",
    "\n",
    "##import files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09481dd-9747-4663-8591-f470375f62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee37b61a-b150-4bf7-b16b-ca7685251210",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd1f830-faa1-449b-99ec-a9dac336049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##note KNN or other clusters might be helpful group the teams in smart way ... but not now.\n",
    "#models\n",
    "\n",
    "##regression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#classifiers (non-tree)\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "##regression models\n",
    "lr = Ridge(alpha=0.001) \n",
    "rfr = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "xgbr = XGBRegressor()\n",
    "\n",
    "##classifier models\n",
    "lrc = RidgeClassifier()\n",
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state = 0)\n",
    "svc = SVC()\n",
    "\n",
    "#tree-based classifiers\n",
    "rfc =  RandomForestClassifier(max_depth=3, random_state=0)\n",
    "bc = BaggingClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "xgbc = XGBClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d23f97-3b91-4868-b566-bed9d7dd76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd905eb-ae20-4c09-a097-d9ced6c6931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a54a87-fb7f-4642-a65e-b2a6c2a6b530",
   "metadata": {},
   "source": [
    "##TUNING INFO \n",
    "\n",
    "\n",
    "##hyper_parameters from here \n",
    "##https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "##for xgboost from here \n",
    "##https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/\n",
    "\n",
    "#xgb\n",
    "\n",
    "trees = [10, 50, 100, 500, 1000, 5000]  #100  #num of trees\n",
    "max_depth = range(1,11)  ##3-5\n",
    "rates = [0.0001, 0.001, 0.01, 0.1, 1.0]  #0.1\n",
    "subsample in arange(0.1, 1.1, 0.1):  #0.4, 0.5  ##this is 0.1, 0.2 ... 1.0 # % of features to sample\n",
    "\n",
    "\n",
    "#svc \n",
    "kernels in [‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’] #if you use poly, then adjust degree\n",
    "C in [100, 10, 1.0, 0.1, 0.001]\n",
    "\n",
    "#gb\n",
    "\n",
    "learning_rate in [0.001, 0.01, 0.1]\n",
    "n_estimators [10, 100, 1000]\n",
    "subsample in [0.5, 0.7, 1.0]\n",
    "max_depth in [3, 7, 9]\n",
    "\n",
    "\n",
    "#rfc\n",
    "max_features [1 to 20]  #key\n",
    "max_features in [‘sqrt’, ‘log2’]\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "#bc\n",
    "n_estimators in [10, 100, 1000]\n",
    "\n",
    "svm_dic = {'kernels':[‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’]}\n",
    "lrc_dic = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n",
    "lgr_hp_dic = {'solver': [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’], 'penalty' : [‘none’, ‘l1’, ‘l2’, ‘elasticnet’],\n",
    "'C' :[100, 10, 1.0, 0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab795dd-c9bf-4524-abdc-c3b9ba2e5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('/Users/joejohns/data_bootcamp/GitHub/final_project_nhl_prediction/Data/Shaped_Data/data_LJ.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d991107-60b5-4e81-a9ba-12814dfe9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X['season'] = X['season'].apply(int)\n",
    "X['game_id'] = X['game_id'].apply(int)\n",
    "X['mp_date'] = X['mp_date'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "803c4a79-77ee-4168-abfe-ac2fe7ba1b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    1218\n",
       "20142015    1180\n",
       "20152016    1180\n",
       "20112012    1177\n",
       "20162017    1172\n",
       "20132014    1166\n",
       "20092010    1108\n",
       "20082009    1093\n",
       "20102011    1091\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d21102ad-152a-4535-a93b-d2a7f72e15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['Unnamed: 0', 'game_id', 'mp_date', 'season', 'home_team', 'away_team',\n",
    "       'home_odds', 'away_odds', 'home_goals', 'away_goals', 'home_win',\n",
    "       'settled_in', ]\n",
    "\n",
    "x_cols = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']\n",
    "columns_to_scale = ['CF%', 'CSh%', 'CSv%', 'FF%', 'FSh%', 'FSv%', 'GDiff',\n",
    "       'GF%', 'PDO', 'PENDiff', 'SF%', 'SDiff', 'Sh%', 'Sv']  ##same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4c9ae08-71a1-45e1-aeb8-4d710ee856a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = np.array(X.loc[(X['season'] <= 20152016), x_cols].copy())\n",
    "  #features test-train\n",
    "Y = X.loc[(X['season'] <= 20152016), y_cols].copy()\n",
    "   #targets\n",
    "y = np.array(Y['home_win']).reshape(-1,1)\n",
    "\n",
    "\n",
    "              \n",
    "x_16 = X.loc[(X['season'] == 20162017), x_cols].copy()  #features test-train\n",
    "Y_16 = X.loc[(X['season'] == 20162017), y_cols].copy()   #targets\n",
    "y_16 = np.array(Y_16['home_win']).reshape(-1,1)\n",
    "\n",
    "x_17 = X.loc[(X['season'] > 20162017), x_cols].copy()  #features test-train\n",
    "Y_17 = X.loc[(X['season'] > 20162017), y_cols].copy()   #targets\n",
    "y_17 = np.array(Y_17['home_win']).reshape(-1,1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fc4a41e-da80-40f0-9e38-47f024dc9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for leakage?? 20172018 in the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "399f8bbc-785d-4d11-a5bd-2ee9bf4b5246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    1218\n",
       "20162017    1172\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] >= 20162017), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3749c2a0-fe5a-40a8-83d1-ac8e285d4d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20142015    1180\n",
       "20152016    1180\n",
       "20112012    1177\n",
       "20132014    1166\n",
       "20092010    1108\n",
       "20082009    1093\n",
       "20102011    1091\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c56437cc-3da4-41c2-9b34-6f9146c5c9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7995"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[(X['season'] <= 20152016), :]['season'].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f62af78-5775-422b-a01a-a0f4da5c7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10385, 26)\n",
      "(7995, 14) (7995, 12) (7995, 1)\n",
      "(1172, 14) (1172, 1)\n",
      "(1218, 14) (1218, 1)\n",
      "sum  7995 1218  is:  9213\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(x.shape, Y.shape, y.shape)\n",
    "print(x_16.shape, y_16.shape)\n",
    "print(x_17.shape, y_17.shape)\n",
    "print('sum ', y.shape[0], y_17.shape[0], ' is: ', y.shape[0] + y_17.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce6940c-1df9-4d1a-aeac-087fa6da4940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20142015    1180\n",
       "20152016    1180\n",
       "20112012    1177\n",
       "20132014    1166\n",
       "20092010    1108\n",
       "20082009    1093\n",
       "20102011    1091\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d07198-8efe-4156-8986-09e21000c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20162017    1172\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_16['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75550a45-7737-4ea8-9184-1f433157700d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20172018    1218\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_17['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9ecd118-f9dc-456b-8b1d-95f1e50d1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "std_scal = StandardScaler()\n",
    "mm_scal = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0df089-4313-488a-994f-1a628ad0117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e6d1c92-3ca5-447e-a9da-9393db3b9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#do standard/minmax scaling on X_train numeric columns ... better to do pipeline? \n",
    "x_train_sc = std_scal.fit_transform(x_train)\n",
    "    \n",
    "#fit the scaler from train portion to the test portion \n",
    "x_test_sc = std_scal.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38b8f56c-f0f0-4672-ac9c-50cdeed0696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81945535, -0.64531481, -0.89537096, ...,  0.38495358,\n",
       "        -0.7379108 , -0.59449417],\n",
       "       [ 0.44947118, -1.12931148, -0.18388693, ...,  0.26086513,\n",
       "        -1.09202027, -0.1653348 ],\n",
       "       [-0.41453811,  0.58089403,  0.57988599, ..., -0.19952102,\n",
       "         0.78306562,  0.33846099],\n",
       "       ...,\n",
       "       [ 0.38958935,  1.10233979, -3.40409108, ...,  0.19432494,\n",
       "         0.78009786, -2.05923379],\n",
       "       [ 1.06325995, -0.2193625 , -0.44962368, ...,  0.72484804,\n",
       "         0.08029749, -0.55717597],\n",
       "       [ 0.01318926,  0.01016254,  1.1048495 , ..., -0.33439977,\n",
       "        -0.20504154,  1.6119448 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65857f1c-005a-411c-b8d2-2f1430170d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42)  TEST error  0.5791119449656035 f1 0.6725060827250607\n",
      " training error  LogisticRegression(max_iter=1000, random_state=42) 0.5737961225766104 f1 0.6725060827250607\n",
      "SVC(random_state=43)  TEST error  0.5666041275797373 f1 0.6701570680628272\n",
      " training error  SVC(random_state=43) 0.5975609756097561 f1 0.6701570680628272\n",
      "[17:23:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joejohns/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)  TEST error  0.5365853658536586 f1 0.6056412985630655\n",
      " training error  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None) 0.9607567229518449 f1 0.6056412985630655\n",
      "SVC(probability=True)  TEST error  0.5666041275797373 f1 0.6701570680628272\n",
      " training error  SVC(probability=True) 0.5975609756097561 f1 0.6701570680628272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgbq\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "clf_A = LogisticRegression(random_state = 42, max_iter = 1000)\n",
    "clf_B = SVC(random_state = 43, kernel = 'rbf')\n",
    "clf_C = xgb.XGBClassifier(seed = 44)\n",
    "clf_D = SVC(probability=True)\n",
    "\n",
    "for model in [clf_A, clf_B, clf_C, clf_D]: # [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "   \n",
    "    model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred = model.predict(x_test_sc)\n",
    "  \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    \n",
    "    print(model, ' TEST error ', acc, 'f1', f1)\n",
    "    \n",
    "    y_pred_train_err =  model.predict(x_train_sc) #! careful with this code\n",
    "    f1_train_err = f1_score(y_train,y_pred_train_err)\n",
    "    acc_train_err = accuracy_score(y_train, y_pred_train_err)\n",
    "    print(' training error ', model, acc_train_err, 'f1', f1) #f1_train_err)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2157c22-d2ff-454e-b3b7-f2bfeb9a11c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42)  TEST error on 20172018  0.583743842364532 f1 0.696588868940754\n",
      "SVC(random_state=43)  TEST error on 20172018  0.5878489326765188 f1 0.7057444314185228\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)  TEST error on 20172018  0.5451559934318555 f1 0.6241519674355496\n",
      "SVC(probability=True)  TEST error on 20172018  0.5878489326765188 f1 0.7057444314185228\n"
     ]
    }
   ],
   "source": [
    "#Now check on X_18 \n",
    "#Now check on X_18 \n",
    "x_17_sc = std_scal.transform(x_17)\n",
    "x_test2 = x_17_sc.copy()\n",
    "y_test2 = y_17\n",
    "\n",
    "\n",
    "for model in [clf_A, clf_B, clf_C, clf_D]: # [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "   \n",
    "    #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred2 = model.predict(x_test2)\n",
    "  \n",
    "    acc = accuracy_score(y_test2, y_pred2)\n",
    "    f1 = f1_score(y_test2,y_pred2)\n",
    "    \n",
    "    print(model, ' TEST error on 20172018 ', acc, 'f1', f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "698d516c-a8ef-40c2-90e3-7bfed5f26276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=42)  TEST error on 20162017  0.5861774744027304 f1 0.6856772521062864\n",
      "SVC(random_state=43)  TEST error on 20162017  0.5878839590443686 f1 0.6893890675241157\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=44,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=44,\n",
      "              subsample=1, tree_method='exact', validate_parameters=1,\n",
      "              verbosity=None)  TEST error on 20162017  0.552901023890785 f1 0.61863173216885\n",
      "SVC(probability=True)  TEST error on 20162017  0.5878839590443686 f1 0.6893890675241157\n"
     ]
    }
   ],
   "source": [
    "x#Now check on X_18 \n",
    "x_16_sc = std_scal.transform(x_16)\n",
    "x_test3 = x_16_sc.copy()\n",
    "y_test3 = y_16.copy()\n",
    "\n",
    "\n",
    "for model in [clf_A, clf_B, clf_C, clf_D]: # [lrc,  gnb, lgr, svc,  rfc, bc, gbc, xgbc]:\n",
    "   \n",
    "    #model.fit(x_train_sc, y_train.ravel())\n",
    "    y_pred3 = model.predict(x_test3)\n",
    "  \n",
    "    acc = accuracy_score(y_test3, y_pred3)\n",
    "    f1 = f1_score(y_test3,y_pred3)\n",
    "    \n",
    "    print(model, ' TEST error on 20162017 ', acc, 'f1', f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d55757-72f6-489f-a125-712cc1d856c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31d9c7-b363-433a-9790-38d456d41fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
